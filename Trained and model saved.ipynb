{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd548961-cde7-42c2-bd94-218e782501c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3052504d-5151-4174-b090-bf1105697b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"test.json\",\"r\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8097871-3421-477b-b6d6-601978105e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9eee845-b27b-4014-be1f-e5a1a7faa805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '190315_E001_13',\n",
       " 'tag': 'meeting',\n",
       " 'title': 'Meeting: Market update meeting',\n",
       " 'original_language': 'en',\n",
       " 'conversation': [{'no': 1,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'How is it going, Wayne?',\n",
       "   'ja_sentence': 'ウェイン、調子はどうです？'},\n",
       "  {'no': 2,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': \"I'm not too bad.\",\n",
       "   'ja_sentence': 'まあまあです。'},\n",
       "  {'no': 3,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'Thank you very much for coming out today.',\n",
       "   'ja_sentence': '今日はご足労ありがとう。'},\n",
       "  {'no': 4,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': \"How's business lately?\",\n",
       "   'ja_sentence': '景気はどうです？'},\n",
       "  {'no': 5,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': \"It's been good.\",\n",
       "   'ja_sentence': 'おかげさまで、順調です。'},\n",
       "  {'no': 6,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': \"We recently commissioned a new facility so I've been busy managing that.\",\n",
       "   'ja_sentence': '最近、新しい施設が稼働開始しまして、その管理で忙しくて。'},\n",
       "  {'no': 7,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'I read about that on your company website.',\n",
       "   'ja_sentence': 'ああ、それ、御社のサイトで読みましたよ。'},\n",
       "  {'no': 8,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'Congratulations.',\n",
       "   'ja_sentence': 'おめでとうございます。'},\n",
       "  {'no': 9,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Thank you.',\n",
       "   'ja_sentence': 'どうも。'},\n",
       "  {'no': 10,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'The Japanese market has been very interested in our product.',\n",
       "   'ja_sentence': '日本市場が当社製品に興味をもっているようです。'},\n",
       "  {'no': 11,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'So having this new facility should satisfy their demand for the next couple of years.',\n",
       "   'ja_sentence': '新施設のおかげで、今後数年間の需要は賄えるかと思います。'},\n",
       "  {'no': 12,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'The Japanese market is trending upwards lately.',\n",
       "   'ja_sentence': '日本市場は最近上向きだね。'},\n",
       "  {'no': 13,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'I was in Japan last month and talked to a couple of potential customers.',\n",
       "   'ja_sentence': '先月日本に出張して、見込みのありそうな数社とミーティングしてきました。'},\n",
       "  {'no': 14,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'They all told me that they are looking for a reliable and consistent supply.',\n",
       "   'ja_sentence': '全社、声を揃えて、信頼できて一貫性のある供給を望んでいましたね。'},\n",
       "  {'no': 15,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Well, we are well positioned then.',\n",
       "   'ja_sentence': 'では、当社はいい状況にあるってわけですね。'},\n",
       "  {'no': 16,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'We have 6 facilities with over 1 million ton capacity.',\n",
       "   'ja_sentence': '出力百万トン以上の工場が6件ありますし。'},\n",
       "  {'no': 17,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'You also operate your own terminal.',\n",
       "   'ja_sentence': '端末も独自に管理されてますしね。'},\n",
       "  {'no': 18,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Yes, so we can control the ship loading and schedule.',\n",
       "   'ja_sentence': 'そう、出荷量やスケジュールも管理できます。'},\n",
       "  {'no': 19,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Having control over the supply chain is definitely helpful.',\n",
       "   'ja_sentence': 'サプライチェーンを管理できるのはとても助かります。'},\n",
       "  {'no': 20,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'Being flexible is really important.',\n",
       "   'ja_sentence': '柔軟性があるのはとても重要です。'},\n",
       "  {'no': 21,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'How has the Korean market been lately?',\n",
       "   'ja_sentence': '韓国市場は最近どんな状況ですか？'},\n",
       "  {'no': 22,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': \"I hear that they aren't looking for long term contracts.\",\n",
       "   'ja_sentence': '長期の契約は望んでいないと聞きましたが。'},\n",
       "  {'no': 23,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Is that true?',\n",
       "   'ja_sentence': 'そうなんでしょうか？'},\n",
       "  {'no': 24,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'Yes it is.',\n",
       "   'ja_sentence': '本当みたいです。'},\n",
       "  {'no': 25,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'They are mainly looking for spot contract.',\n",
       "   'ja_sentence': '現物契約中心のようです。'},\n",
       "  {'no': 26,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Why is that?',\n",
       "   'ja_sentence': 'どうしてなんでしょう？'},\n",
       "  {'no': 27,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': \"Well they don't want to overcommit to one supplier.\",\n",
       "   'ja_sentence': 'サプライヤー一社に傾倒したくないんでしょうね。'},\n",
       "  {'no': 28,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'So they prefer to source from various suppliers.',\n",
       "   'ja_sentence': 'なので、いろいろなところから調達したいんですよ。'},\n",
       "  {'no': 29,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Is that to also keep the price down?',\n",
       "   'ja_sentence': '価格を抑える意味もあるんでしょうか？'},\n",
       "  {'no': 30,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'They can compare prices and shop around for the cheapest price.',\n",
       "   'ja_sentence': '価格を比較して、一番安いところから購入するのです。'},\n",
       "  {'no': 31,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Sounds like a lot of work.',\n",
       "   'ja_sentence': 'よっぽど、大変そうですがね。'},\n",
       "  {'no': 32,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'We could offer them a 10-year contract and they can sit back and relax.',\n",
       "   'ja_sentence': '10年契約をオファーすれば、向こうはのんびりリラックスできるのに。'},\n",
       "  {'no': 33,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'Yeah, that would make life easier.',\n",
       "   'ja_sentence': 'その方が楽でしょうに。'},\n",
       "  {'no': 34,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'We should keep an eye on that market.',\n",
       "   'ja_sentence': '韓国市場からは目が離せないですね。'},\n",
       "  {'no': 35,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Yeah, there is a lot of changes happening lately.',\n",
       "   'ja_sentence': 'そう、最近、いろいろ変化が起きていますからね。'},\n",
       "  {'no': 36,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'We should catch up again in a couple months?',\n",
       "   'ja_sentence': '２ヶ月後くらいにまた、情報交換しましょう。'},\n",
       "  {'no': 37,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'That would be good.',\n",
       "   'ja_sentence': 'いいですね。'},\n",
       "  {'no': 38,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'Great.',\n",
       "   'ja_sentence': '良かった。'},\n",
       "  {'no': 39,\n",
       "   'en_speaker': 'Mr. John Smith',\n",
       "   'ja_speaker': 'ジョン スミスさん',\n",
       "   'en_sentence': 'Thank you for your time today.',\n",
       "   'ja_sentence': '今日はお時間をありがとうございました。'},\n",
       "  {'no': 40,\n",
       "   'en_speaker': 'Mr. Wayne Willis',\n",
       "   'ja_speaker': 'ウェイン ウィリスさん',\n",
       "   'en_sentence': 'Thank you as well.',\n",
       "   'ja_sentence': 'こちらこそ。'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac905ad3-e445-4bba-8b8a-472c76c354c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "english=[]\n",
    "japanese=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd21f0cb-be3c-47c9-9eb0-460b5adef114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dat in data:\n",
    "    for j in dat['conversation']:\n",
    "        english.append(j['en_sentence'])\n",
    "        japanese.append(j['ja_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b50013e7-0cf1-462c-abc6-f16eccc2235c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a1e8612-ef7a-4852-8300-1f479e35c64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(japanese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d537196-2d07-48bd-bd2c-9a65e3a049cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_eng=0\n",
    "mx_jp=0\n",
    "for i in range(len(english)):\n",
    "    mx_eng=max(mx_eng,len(english[i]))\n",
    "for i in range(len(japanese)):\n",
    "    mx_jp=max(mx_jp,len(japanese[i]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09703c-0545-4f5a-b1ce-1e0a77f4b9ae",
   "metadata": {},
   "source": [
    "Max of english and Max of Japanese tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61934533-df74-495b-9037-49536ca19c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 95\n"
     ]
    }
   ],
   "source": [
    "print(mx_eng,mx_jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a44cd-a876-48e0-aa22-b97607712c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a9e297-01b3-422c-aed3-d55a3fd89c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82cfecef-5686-4af6-bd60-7580856cfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    \"bos_token\": \"<sos>\",\n",
    "    \"eos_token\": \"<eos>\",\n",
    "    \"pad_token\": \"<pad>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bbb6e46-f170-4be7-8bfe-be241bd0e8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f012e8fd-06b9-45f7-a232-cab40e3faec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jptokids=[]\n",
    "jptorch=[]\n",
    "for i in range(len(japanese)):\n",
    "    ax = tokenizer.tokenize(japanese[i])\n",
    "    ax = [tokenizer.bos_token] + ax + [tokenizer.eos_token]  \n",
    "    jptokids.append(tokenizer.convert_tokens_to_ids(ax))\n",
    "    jptorch.append(torch.tensor(jptokids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcdb36-b3c6-4dfb-b9d7-bb612b8594e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fb22e53-41df-4ef0-b7d7-dbb1f59c0c99",
   "metadata": {},
   "source": [
    "This is using tokenizer.encode to automatically add eos and sos \n",
    "\n",
    "for i in range(len(japanese)):\n",
    "    jptokids.append(tokenizer.encode(japanese[i], add_special_tokens=True))  \n",
    "    print(jptokids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba07688-66ed-4cbe-8e6e-406442b50b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c633c9-41dd-4421-b052-c10bc4288998",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_jp=0\n",
    "vocabsize_jp=0\n",
    "for i in range(len(jptokids)):\n",
    "    maxlen_jp=max(maxlen_jp,len(jptokids[i]))\n",
    "    vocabsize_jp = max(vocabsize_jp,max(jptokids[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f412545b-77cf-490b-974e-270ca3dcc762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 32001)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_jp,vocabsize_jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3141c88b-b634-4ff8-9658-e725d09479e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engtokenizer=BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5d8a15b-1ffb-44f6-bf9e-9b828c0596e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engtokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1562fb92-99e4-452f-888e-3bcd96083864",
   "metadata": {},
   "outputs": [],
   "source": [
    "engtokids=[]\n",
    "engtorch=[]\n",
    "for i in range(len(english)):\n",
    "    ax=engtokenizer.tokenize(english[i])\n",
    "    ax=[engtokenizer.bos_token]+ax+[engtokenizer.eos_token]\n",
    "    engtokids.append(engtokenizer.convert_tokens_to_ids(ax))\n",
    "    engtorch.append(torch.tensor(engtokids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72bee0a8-d868-4c26-b3c9-e19fcafa76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_eng=0\n",
    "vocabsize_eng=0\n",
    "for i in range(len(engtokids)):\n",
    "    maxlen_eng=max(maxlen_eng,len(engtokids[i]))\n",
    "    vocabsize_eng = max(vocabsize_eng,max(engtokids[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "993addb7-2629-4bd6-a678-bc8b2009308f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 30523, 59, 32001)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_eng,vocabsize_eng,maxlen_jp,vocabsize_jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d73fda59-747c-4592-8743-aa05928fad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> ウェイン 、 調子 は どう です ? <eos>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(jptokids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18b4eb58-0c15-4998-bf7a-d8030eb3a34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how is it going, wayne?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engtokenizer.decode(engtokids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d462bd63-bdb3-4cd9-b6a1-ad0ee9b1fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engpadded= pad_sequence(engtorch, batch_first=True, padding_value=engtokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d8d9227-d8c6-4756-95ee-712e7f0b56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jppadded= pad_sequence(jptorch,batch_first=True ,padding_value=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1de242f9-8acd-4daa-83f8-aadd9ba848df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, en_tensors, jp_tensors):\n",
    "        self.en_tensors = en_tensors\n",
    "        self.jp_tensors = jp_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return English and Japanese tensors for each sentence pair\n",
    "        return self.en_tensors[idx], self.jp_tensors[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ed2b937-b89c-4629-bd11-9e5d2b90bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(en_tensors, jp_tensors, batch_size=32):\n",
    "    dataset = TranslationDataset(en_tensors, jp_tensors)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b641f289-cbec-4d4c-a6d3-af0bcfdc9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(engpadded, jppadded, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5d3c0-fc2c-4164-8452-99e6bfa3ca12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0ca2579-16ec-495a-8c03-8490debb0668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30522, 23156,  1012,  ..., 30524, 30524, 30524],\n",
      "        [30522,  1045,  2064,  ..., 30524, 30524, 30524],\n",
      "        [30522,  2031,  1037,  ..., 30524, 30524, 30524],\n",
      "        ...,\n",
      "        [30522,  2057,  2024,  ..., 30524, 30524, 30524],\n",
      "        [30522,  1045,  2123,  ..., 30524, 30524, 30524],\n",
      "        [30522,  4995,  1005,  ..., 30524, 30524, 30524]])\n",
      "tensor([[32000,    73,  2087,  ..., 32002, 32002, 32002],\n",
      "        [32000,    44, 14027,  ..., 32002, 32002, 32002],\n",
      "        [32000, 13069,    52,  ..., 32002, 32002, 32002],\n",
      "        ...,\n",
      "        [32000,  5330,    13,  ..., 32002, 32002, 32002],\n",
      "        [32000,  1276, 29323,  ..., 32002, 32002, 32002],\n",
      "        [32000,    91,  2612,  ..., 32002, 32002, 32002]])\n"
     ]
    }
   ],
   "source": [
    "for i in next(iter(train_loader)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "120b1adc-2626-455f-8a91-63ccf7ff4b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45])\n",
      "torch.Size([59])\n"
     ]
    }
   ],
   "source": [
    "for i in next(iter(train_loader)):\n",
    "    print(i[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e06aaf3-0d42-4c0b-aa48-e4c4d719616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eencoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        ot, hidden = self.rnn(embedded)\n",
    "        #return ot\n",
    "        return hidden.transpose(0,1) # Transpose to (batch_size, num_layers * num_directions, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "449c7fae-58ba-48df-b287-ce86237d5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)  # batch_first=True\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1) # unsqueeze to (batch_size, 1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.rnn(embedded, hidden.transpose(0,1)) # Transpose hidden to (num_layers * num_directions, batch_size, hidden_size)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden.transpose(0,1) # Transpose hidden back to (batch_size, num_layers * num_directions, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e63d335c-4a47-4633-b7dd-4ab5ef0e9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source_sequence, target_sequence):\n",
    "        batch_size = target_sequence.shape[0]\n",
    "        target_sequence_length = target_sequence.shape[1]\n",
    "        target_vocabulary_size = self.decoder.output_dim\n",
    "\n",
    "        decoder_outputs = torch.zeros(batch_size, target_sequence_length, target_vocabulary_size).to(source_sequence.device)\n",
    "\n",
    "        encoder_final_hidden = self.encoder(source_sequence)\n",
    "\n",
    "        decoder_input = target_sequence[:, 0]\n",
    "\n",
    "        for time_step in range(1, target_sequence_length):\n",
    "            decoder_prediction, decoder_hidden = self.decoder(decoder_input, encoder_final_hidden)\n",
    "\n",
    "            decoder_outputs[:, time_step] = decoder_prediction\n",
    "\n",
    "            # Always use teacher forcing: use the actual target token\n",
    "            decoder_input = target_sequence[:, time_step]\n",
    "\n",
    "            encoder_final_hidden = decoder_hidden\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c8a739d-aedb-4820-9470-c0687a89e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "engenc=eencoder(input_dim=31000,emb_dim=30,hid_dim=800,dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e3205a4-fe60-4e75-a19f-976b852c2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpdec=deecoder(output_dim=32100,emb_dim=30,hid_dim=800,dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "684f944b-2154-446e-b2a5-fe1524ee39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(engenc, jpdec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7053885-3c13-4578-b041-d36bb66c02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a7b97e6-be5e-4626-bbd3-94db22b1274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, src, trg, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, trg)\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output[:, 1:].reshape(-1, output_dim)\n",
    "    trg = trg[:, 1:].reshape(-1)\n",
    "    loss = criterion(output, trg)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d3dbd-8d24-4c71-b40c-d8af4b8f6322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e544567e-468d-452a-bd75-52b7ee81aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 10.382\n",
      "Epoch: 01 | Train Loss: 10.324\n",
      "Epoch: 01 | Train Loss: 10.265\n",
      "Epoch: 01 | Train Loss: 10.112\n",
      "Epoch: 01 | Train Loss: 9.532\n",
      "Epoch: 01 | Train Loss: 7.458\n",
      "Epoch: 01 | Train Loss: 6.702\n",
      "Epoch: 01 | Train Loss: 6.374\n",
      "Epoch: 01 | Train Loss: 6.282\n",
      "Epoch: 01 | Train Loss: 6.475\n",
      "Epoch: 01 | Train Loss: 6.451\n",
      "Epoch: 01 | Train Loss: 6.678\n",
      "Epoch: 01 | Train Loss: 6.632\n",
      "Epoch: 01 | Train Loss: 6.969\n",
      "Epoch: 01 | Train Loss: 6.994\n",
      "Epoch: 01 | Train Loss: 6.272\n",
      "Epoch: 01 | Train Loss: 6.307\n",
      "Epoch: 01 | Train Loss: 6.357\n",
      "Epoch: 01 | Train Loss: 6.271\n",
      "Epoch: 01 | Train Loss: 6.206\n",
      "Epoch: 01 | Train Loss: 6.512\n",
      "Epoch: 01 | Train Loss: 6.097\n",
      "Epoch: 01 | Train Loss: 6.088\n",
      "Epoch: 01 | Train Loss: 6.242\n",
      "Epoch: 01 | Train Loss: 5.990\n",
      "Epoch: 01 | Train Loss: 6.097\n",
      "Epoch: 01 | Train Loss: 5.928\n",
      "Epoch: 01 | Train Loss: 6.355\n",
      "Epoch: 01 | Train Loss: 6.045\n",
      "Epoch: 01 | Train Loss: 6.080\n",
      "Epoch: 01 | Train Loss: 6.064\n",
      "Epoch: 01 | Train Loss: 6.151\n",
      "Epoch: 01 | Train Loss: 6.197\n",
      "Epoch: 01 | Train Loss: 6.191\n",
      "Epoch: 01 | Train Loss: 5.834\n",
      "Epoch: 01 | Train Loss: 6.305\n",
      "Epoch: 01 | Train Loss: 6.036\n",
      "Epoch: 01 | Train Loss: 6.235\n",
      "Epoch: 01 | Train Loss: 5.934\n",
      "Epoch: 01 | Train Loss: 5.755\n",
      "Epoch: 01 | Train Loss: 5.907\n",
      "Epoch: 01 | Train Loss: 5.933\n",
      "Epoch: 01 | Train Loss: 6.114\n",
      "Epoch: 01 | Train Loss: 6.142\n",
      "Epoch: 01 | Train Loss: 6.137\n",
      "Epoch: 01 | Train Loss: 5.945\n",
      "Epoch: 01 | Train Loss: 6.038\n",
      "Epoch: 01 | Train Loss: 5.733\n",
      "Epoch: 01 | Train Loss: 5.834\n",
      "Epoch: 01 | Train Loss: 5.806\n",
      "Epoch: 01 | Train Loss: 5.962\n",
      "Epoch: 01 | Train Loss: 6.223\n",
      "Epoch: 01 | Train Loss: 5.854\n",
      "Epoch: 01 | Train Loss: 5.585\n",
      "Epoch: 01 | Train Loss: 5.716\n",
      "Epoch: 01 | Train Loss: 5.754\n",
      "Epoch: 01 | Train Loss: 5.930\n",
      "Epoch: 01 | Train Loss: 5.843\n",
      "Epoch: 01 | Train Loss: 5.787\n",
      "Epoch: 01 | Train Loss: 5.856\n",
      "Epoch: 01 | Train Loss: 5.731\n",
      "Epoch: 01 | Train Loss: 5.551\n",
      "Epoch: 01 | Train Loss: 5.619\n",
      "Epoch: 01 | Train Loss: 5.983\n",
      "Epoch: 01 | Train Loss: 5.765\n",
      "Epoch: 01 | Train Loss: 5.800\n",
      "Epoch: 01 | Train Loss: 6.042\n",
      "Epoch: 02 | Train Loss: 5.333\n",
      "Epoch: 02 | Train Loss: 5.288\n",
      "Epoch: 02 | Train Loss: 5.313\n",
      "Epoch: 02 | Train Loss: 5.380\n",
      "Epoch: 02 | Train Loss: 5.579\n",
      "Epoch: 02 | Train Loss: 5.401\n",
      "Epoch: 02 | Train Loss: 5.482\n",
      "Epoch: 02 | Train Loss: 5.286\n",
      "Epoch: 02 | Train Loss: 5.253\n",
      "Epoch: 02 | Train Loss: 5.503\n",
      "Epoch: 02 | Train Loss: 5.175\n",
      "Epoch: 02 | Train Loss: 5.293\n",
      "Epoch: 02 | Train Loss: 5.374\n",
      "Epoch: 02 | Train Loss: 5.305\n",
      "Epoch: 02 | Train Loss: 5.243\n",
      "Epoch: 02 | Train Loss: 5.211\n",
      "Epoch: 02 | Train Loss: 5.288\n",
      "Epoch: 02 | Train Loss: 5.107\n",
      "Epoch: 02 | Train Loss: 5.265\n",
      "Epoch: 02 | Train Loss: 5.547\n",
      "Epoch: 02 | Train Loss: 5.347\n",
      "Epoch: 02 | Train Loss: 5.444\n",
      "Epoch: 02 | Train Loss: 5.283\n",
      "Epoch: 02 | Train Loss: 5.138\n",
      "Epoch: 02 | Train Loss: 5.242\n",
      "Epoch: 02 | Train Loss: 5.461\n",
      "Epoch: 02 | Train Loss: 5.240\n",
      "Epoch: 02 | Train Loss: 5.372\n",
      "Epoch: 02 | Train Loss: 5.280\n",
      "Epoch: 02 | Train Loss: 5.128\n",
      "Epoch: 02 | Train Loss: 5.391\n",
      "Epoch: 02 | Train Loss: 5.084\n",
      "Epoch: 02 | Train Loss: 5.214\n",
      "Epoch: 02 | Train Loss: 5.299\n",
      "Epoch: 02 | Train Loss: 5.303\n",
      "Epoch: 02 | Train Loss: 5.088\n",
      "Epoch: 02 | Train Loss: 5.082\n",
      "Epoch: 02 | Train Loss: 5.221\n",
      "Epoch: 02 | Train Loss: 5.155\n",
      "Epoch: 02 | Train Loss: 5.120\n",
      "Epoch: 02 | Train Loss: 5.262\n",
      "Epoch: 02 | Train Loss: 4.785\n",
      "Epoch: 02 | Train Loss: 4.990\n",
      "Epoch: 02 | Train Loss: 5.067\n",
      "Epoch: 02 | Train Loss: 5.096\n",
      "Epoch: 02 | Train Loss: 5.508\n",
      "Epoch: 02 | Train Loss: 5.077\n",
      "Epoch: 02 | Train Loss: 4.979\n",
      "Epoch: 02 | Train Loss: 4.961\n",
      "Epoch: 02 | Train Loss: 5.164\n",
      "Epoch: 02 | Train Loss: 5.216\n",
      "Epoch: 02 | Train Loss: 5.114\n",
      "Epoch: 02 | Train Loss: 5.382\n",
      "Epoch: 02 | Train Loss: 5.135\n",
      "Epoch: 02 | Train Loss: 5.127\n",
      "Epoch: 02 | Train Loss: 5.159\n",
      "Epoch: 02 | Train Loss: 5.140\n",
      "Epoch: 02 | Train Loss: 4.846\n",
      "Epoch: 02 | Train Loss: 4.925\n",
      "Epoch: 02 | Train Loss: 4.882\n",
      "Epoch: 02 | Train Loss: 5.178\n",
      "Epoch: 02 | Train Loss: 5.077\n",
      "Epoch: 02 | Train Loss: 4.918\n",
      "Epoch: 02 | Train Loss: 4.981\n",
      "Epoch: 02 | Train Loss: 5.104\n",
      "Epoch: 02 | Train Loss: 5.156\n",
      "Epoch: 02 | Train Loss: 5.066\n",
      "Epoch: 03 | Train Loss: 4.809\n",
      "Epoch: 03 | Train Loss: 4.839\n",
      "Epoch: 03 | Train Loss: 4.651\n",
      "Epoch: 03 | Train Loss: 4.677\n",
      "Epoch: 03 | Train Loss: 4.612\n",
      "Epoch: 03 | Train Loss: 4.907\n",
      "Epoch: 03 | Train Loss: 4.878\n",
      "Epoch: 03 | Train Loss: 4.771\n",
      "Epoch: 03 | Train Loss: 4.839\n",
      "Epoch: 03 | Train Loss: 4.651\n",
      "Epoch: 03 | Train Loss: 4.921\n",
      "Epoch: 03 | Train Loss: 4.814\n",
      "Epoch: 03 | Train Loss: 4.987\n",
      "Epoch: 03 | Train Loss: 4.904\n",
      "Epoch: 03 | Train Loss: 4.703\n",
      "Epoch: 03 | Train Loss: 4.779\n",
      "Epoch: 03 | Train Loss: 4.816\n",
      "Epoch: 03 | Train Loss: 4.604\n",
      "Epoch: 03 | Train Loss: 4.612\n",
      "Epoch: 03 | Train Loss: 4.878\n",
      "Epoch: 03 | Train Loss: 4.905\n",
      "Epoch: 03 | Train Loss: 4.731\n",
      "Epoch: 03 | Train Loss: 5.095\n",
      "Epoch: 03 | Train Loss: 4.522\n",
      "Epoch: 03 | Train Loss: 4.695\n",
      "Epoch: 03 | Train Loss: 4.838\n",
      "Epoch: 03 | Train Loss: 4.829\n",
      "Epoch: 03 | Train Loss: 4.798\n",
      "Epoch: 03 | Train Loss: 4.907\n",
      "Epoch: 03 | Train Loss: 4.827\n",
      "Epoch: 03 | Train Loss: 4.786\n",
      "Epoch: 03 | Train Loss: 4.744\n",
      "Epoch: 03 | Train Loss: 4.511\n",
      "Epoch: 03 | Train Loss: 4.871\n",
      "Epoch: 03 | Train Loss: 5.072\n",
      "Epoch: 03 | Train Loss: 4.598\n",
      "Epoch: 03 | Train Loss: 4.948\n",
      "Epoch: 03 | Train Loss: 4.770\n",
      "Epoch: 03 | Train Loss: 4.671\n",
      "Epoch: 03 | Train Loss: 4.700\n",
      "Epoch: 03 | Train Loss: 4.768\n",
      "Epoch: 03 | Train Loss: 4.905\n",
      "Epoch: 03 | Train Loss: 4.892\n",
      "Epoch: 03 | Train Loss: 4.704\n",
      "Epoch: 03 | Train Loss: 4.445\n",
      "Epoch: 03 | Train Loss: 4.715\n",
      "Epoch: 03 | Train Loss: 4.342\n",
      "Epoch: 03 | Train Loss: 4.517\n",
      "Epoch: 03 | Train Loss: 4.783\n",
      "Epoch: 03 | Train Loss: 4.360\n",
      "Epoch: 03 | Train Loss: 4.830\n",
      "Epoch: 03 | Train Loss: 4.819\n",
      "Epoch: 03 | Train Loss: 4.723\n",
      "Epoch: 03 | Train Loss: 4.789\n",
      "Epoch: 03 | Train Loss: 4.865\n",
      "Epoch: 03 | Train Loss: 4.745\n",
      "Epoch: 03 | Train Loss: 4.623\n",
      "Epoch: 03 | Train Loss: 4.691\n",
      "Epoch: 03 | Train Loss: 4.558\n",
      "Epoch: 03 | Train Loss: 4.878\n",
      "Epoch: 03 | Train Loss: 4.668\n",
      "Epoch: 03 | Train Loss: 4.704\n",
      "Epoch: 03 | Train Loss: 4.574\n",
      "Epoch: 03 | Train Loss: 4.496\n",
      "Epoch: 03 | Train Loss: 4.695\n",
      "Epoch: 03 | Train Loss: 4.692\n",
      "Epoch: 03 | Train Loss: 4.251\n",
      "Epoch: 04 | Train Loss: 4.584\n",
      "Epoch: 04 | Train Loss: 4.420\n",
      "Epoch: 04 | Train Loss: 4.486\n",
      "Epoch: 04 | Train Loss: 4.521\n",
      "Epoch: 04 | Train Loss: 4.454\n",
      "Epoch: 04 | Train Loss: 4.346\n",
      "Epoch: 04 | Train Loss: 4.519\n",
      "Epoch: 04 | Train Loss: 4.294\n",
      "Epoch: 04 | Train Loss: 4.542\n",
      "Epoch: 04 | Train Loss: 4.418\n",
      "Epoch: 04 | Train Loss: 4.609\n",
      "Epoch: 04 | Train Loss: 4.377\n",
      "Epoch: 04 | Train Loss: 4.583\n",
      "Epoch: 04 | Train Loss: 4.393\n",
      "Epoch: 04 | Train Loss: 4.389\n",
      "Epoch: 04 | Train Loss: 4.291\n",
      "Epoch: 04 | Train Loss: 4.407\n",
      "Epoch: 04 | Train Loss: 4.417\n",
      "Epoch: 04 | Train Loss: 4.333\n",
      "Epoch: 04 | Train Loss: 4.609\n",
      "Epoch: 04 | Train Loss: 4.494\n",
      "Epoch: 04 | Train Loss: 4.291\n",
      "Epoch: 04 | Train Loss: 4.450\n",
      "Epoch: 04 | Train Loss: 4.426\n",
      "Epoch: 04 | Train Loss: 4.459\n",
      "Epoch: 04 | Train Loss: 4.511\n",
      "Epoch: 04 | Train Loss: 4.323\n",
      "Epoch: 04 | Train Loss: 4.322\n",
      "Epoch: 04 | Train Loss: 4.305\n",
      "Epoch: 04 | Train Loss: 4.228\n",
      "Epoch: 04 | Train Loss: 4.360\n",
      "Epoch: 04 | Train Loss: 4.432\n",
      "Epoch: 04 | Train Loss: 4.180\n",
      "Epoch: 04 | Train Loss: 4.424\n",
      "Epoch: 04 | Train Loss: 4.374\n",
      "Epoch: 04 | Train Loss: 4.348\n",
      "Epoch: 04 | Train Loss: 4.388\n",
      "Epoch: 04 | Train Loss: 4.447\n",
      "Epoch: 04 | Train Loss: 4.143\n",
      "Epoch: 04 | Train Loss: 4.284\n",
      "Epoch: 04 | Train Loss: 4.368\n",
      "Epoch: 04 | Train Loss: 4.362\n",
      "Epoch: 04 | Train Loss: 4.247\n",
      "Epoch: 04 | Train Loss: 4.368\n",
      "Epoch: 04 | Train Loss: 4.293\n",
      "Epoch: 04 | Train Loss: 4.408\n",
      "Epoch: 04 | Train Loss: 4.218\n",
      "Epoch: 04 | Train Loss: 4.351\n",
      "Epoch: 04 | Train Loss: 4.360\n",
      "Epoch: 04 | Train Loss: 4.480\n",
      "Epoch: 04 | Train Loss: 4.530\n",
      "Epoch: 04 | Train Loss: 4.262\n",
      "Epoch: 04 | Train Loss: 4.526\n",
      "Epoch: 04 | Train Loss: 4.184\n",
      "Epoch: 04 | Train Loss: 4.288\n",
      "Epoch: 04 | Train Loss: 4.317\n",
      "Epoch: 04 | Train Loss: 4.482\n",
      "Epoch: 04 | Train Loss: 4.229\n",
      "Epoch: 04 | Train Loss: 4.535\n",
      "Epoch: 04 | Train Loss: 4.343\n",
      "Epoch: 04 | Train Loss: 4.504\n",
      "Epoch: 04 | Train Loss: 4.132\n",
      "Epoch: 04 | Train Loss: 4.373\n",
      "Epoch: 04 | Train Loss: 4.212\n",
      "Epoch: 04 | Train Loss: 4.437\n",
      "Epoch: 04 | Train Loss: 4.318\n",
      "Epoch: 04 | Train Loss: 4.270\n",
      "Epoch: 05 | Train Loss: 4.089\n",
      "Epoch: 05 | Train Loss: 4.240\n",
      "Epoch: 05 | Train Loss: 3.810\n",
      "Epoch: 05 | Train Loss: 3.913\n",
      "Epoch: 05 | Train Loss: 3.914\n",
      "Epoch: 05 | Train Loss: 4.028\n",
      "Epoch: 05 | Train Loss: 3.946\n",
      "Epoch: 05 | Train Loss: 3.843\n",
      "Epoch: 05 | Train Loss: 4.270\n",
      "Epoch: 05 | Train Loss: 4.092\n",
      "Epoch: 05 | Train Loss: 4.111\n",
      "Epoch: 05 | Train Loss: 4.088\n",
      "Epoch: 05 | Train Loss: 4.225\n",
      "Epoch: 05 | Train Loss: 4.210\n",
      "Epoch: 05 | Train Loss: 4.032\n",
      "Epoch: 05 | Train Loss: 4.203\n",
      "Epoch: 05 | Train Loss: 4.025\n",
      "Epoch: 05 | Train Loss: 4.073\n",
      "Epoch: 05 | Train Loss: 3.969\n",
      "Epoch: 05 | Train Loss: 4.136\n",
      "Epoch: 05 | Train Loss: 3.910\n",
      "Epoch: 05 | Train Loss: 3.562\n",
      "Epoch: 05 | Train Loss: 3.992\n",
      "Epoch: 05 | Train Loss: 3.958\n",
      "Epoch: 05 | Train Loss: 4.123\n",
      "Epoch: 05 | Train Loss: 4.094\n",
      "Epoch: 05 | Train Loss: 3.800\n",
      "Epoch: 05 | Train Loss: 4.090\n",
      "Epoch: 05 | Train Loss: 4.100\n",
      "Epoch: 05 | Train Loss: 3.672\n",
      "Epoch: 05 | Train Loss: 4.017\n",
      "Epoch: 05 | Train Loss: 4.114\n",
      "Epoch: 05 | Train Loss: 4.053\n",
      "Epoch: 05 | Train Loss: 3.946\n",
      "Epoch: 05 | Train Loss: 3.985\n",
      "Epoch: 05 | Train Loss: 4.204\n",
      "Epoch: 05 | Train Loss: 3.911\n",
      "Epoch: 05 | Train Loss: 4.418\n",
      "Epoch: 05 | Train Loss: 4.022\n",
      "Epoch: 05 | Train Loss: 4.110\n",
      "Epoch: 05 | Train Loss: 4.088\n",
      "Epoch: 05 | Train Loss: 4.171\n",
      "Epoch: 05 | Train Loss: 4.120\n",
      "Epoch: 05 | Train Loss: 3.944\n",
      "Epoch: 05 | Train Loss: 4.022\n",
      "Epoch: 05 | Train Loss: 4.034\n",
      "Epoch: 05 | Train Loss: 3.996\n",
      "Epoch: 05 | Train Loss: 4.164\n",
      "Epoch: 05 | Train Loss: 4.242\n",
      "Epoch: 05 | Train Loss: 4.013\n",
      "Epoch: 05 | Train Loss: 4.146\n",
      "Epoch: 05 | Train Loss: 4.029\n",
      "Epoch: 05 | Train Loss: 4.061\n",
      "Epoch: 05 | Train Loss: 4.205\n",
      "Epoch: 05 | Train Loss: 3.868\n",
      "Epoch: 05 | Train Loss: 4.129\n",
      "Epoch: 05 | Train Loss: 4.336\n",
      "Epoch: 05 | Train Loss: 4.252\n",
      "Epoch: 05 | Train Loss: 4.063\n",
      "Epoch: 05 | Train Loss: 4.113\n",
      "Epoch: 05 | Train Loss: 4.115\n",
      "Epoch: 05 | Train Loss: 3.971\n",
      "Epoch: 05 | Train Loss: 3.983\n",
      "Epoch: 05 | Train Loss: 3.951\n",
      "Epoch: 05 | Train Loss: 3.974\n",
      "Epoch: 05 | Train Loss: 4.017\n",
      "Epoch: 05 | Train Loss: 3.590\n",
      "Epoch: 06 | Train Loss: 3.617\n",
      "Epoch: 06 | Train Loss: 3.643\n",
      "Epoch: 06 | Train Loss: 3.635\n",
      "Epoch: 06 | Train Loss: 3.852\n",
      "Epoch: 06 | Train Loss: 3.598\n",
      "Epoch: 06 | Train Loss: 3.804\n",
      "Epoch: 06 | Train Loss: 3.764\n",
      "Epoch: 06 | Train Loss: 3.664\n",
      "Epoch: 06 | Train Loss: 3.778\n",
      "Epoch: 06 | Train Loss: 3.588\n",
      "Epoch: 06 | Train Loss: 3.761\n",
      "Epoch: 06 | Train Loss: 3.630\n",
      "Epoch: 06 | Train Loss: 3.822\n",
      "Epoch: 06 | Train Loss: 3.781\n",
      "Epoch: 06 | Train Loss: 3.756\n",
      "Epoch: 06 | Train Loss: 3.781\n",
      "Epoch: 06 | Train Loss: 3.814\n",
      "Epoch: 06 | Train Loss: 3.546\n",
      "Epoch: 06 | Train Loss: 3.791\n",
      "Epoch: 06 | Train Loss: 3.716\n",
      "Epoch: 06 | Train Loss: 3.559\n",
      "Epoch: 06 | Train Loss: 3.856\n",
      "Epoch: 06 | Train Loss: 3.767\n",
      "Epoch: 06 | Train Loss: 3.572\n",
      "Epoch: 06 | Train Loss: 3.805\n",
      "Epoch: 06 | Train Loss: 3.681\n",
      "Epoch: 06 | Train Loss: 3.883\n",
      "Epoch: 06 | Train Loss: 3.527\n",
      "Epoch: 06 | Train Loss: 3.806\n",
      "Epoch: 06 | Train Loss: 3.648\n",
      "Epoch: 06 | Train Loss: 3.592\n",
      "Epoch: 06 | Train Loss: 4.080\n",
      "Epoch: 06 | Train Loss: 3.726\n",
      "Epoch: 06 | Train Loss: 3.873\n",
      "Epoch: 06 | Train Loss: 3.874\n",
      "Epoch: 06 | Train Loss: 3.362\n",
      "Epoch: 06 | Train Loss: 3.897\n",
      "Epoch: 06 | Train Loss: 3.590\n",
      "Epoch: 06 | Train Loss: 3.799\n",
      "Epoch: 06 | Train Loss: 3.693\n",
      "Epoch: 06 | Train Loss: 3.698\n",
      "Epoch: 06 | Train Loss: 3.878\n",
      "Epoch: 06 | Train Loss: 3.681\n",
      "Epoch: 06 | Train Loss: 3.800\n",
      "Epoch: 06 | Train Loss: 3.750\n",
      "Epoch: 06 | Train Loss: 3.642\n",
      "Epoch: 06 | Train Loss: 3.708\n",
      "Epoch: 06 | Train Loss: 3.820\n",
      "Epoch: 06 | Train Loss: 3.618\n",
      "Epoch: 06 | Train Loss: 3.796\n",
      "Epoch: 06 | Train Loss: 3.453\n",
      "Epoch: 06 | Train Loss: 3.827\n",
      "Epoch: 06 | Train Loss: 3.837\n",
      "Epoch: 06 | Train Loss: 3.956\n",
      "Epoch: 06 | Train Loss: 3.787\n",
      "Epoch: 06 | Train Loss: 3.679\n",
      "Epoch: 06 | Train Loss: 3.928\n",
      "Epoch: 06 | Train Loss: 3.748\n",
      "Epoch: 06 | Train Loss: 3.857\n",
      "Epoch: 06 | Train Loss: 3.613\n",
      "Epoch: 06 | Train Loss: 3.874\n",
      "Epoch: 06 | Train Loss: 3.701\n",
      "Epoch: 06 | Train Loss: 3.795\n",
      "Epoch: 06 | Train Loss: 3.687\n",
      "Epoch: 06 | Train Loss: 3.734\n",
      "Epoch: 06 | Train Loss: 3.675\n",
      "Epoch: 06 | Train Loss: 3.678\n",
      "Epoch: 07 | Train Loss: 3.415\n",
      "Epoch: 07 | Train Loss: 3.435\n",
      "Epoch: 07 | Train Loss: 3.327\n",
      "Epoch: 07 | Train Loss: 3.267\n",
      "Epoch: 07 | Train Loss: 3.237\n",
      "Epoch: 07 | Train Loss: 3.447\n",
      "Epoch: 07 | Train Loss: 3.182\n",
      "Epoch: 07 | Train Loss: 3.448\n",
      "Epoch: 07 | Train Loss: 3.290\n",
      "Epoch: 07 | Train Loss: 3.318\n",
      "Epoch: 07 | Train Loss: 3.389\n",
      "Epoch: 07 | Train Loss: 3.296\n",
      "Epoch: 07 | Train Loss: 3.389\n",
      "Epoch: 07 | Train Loss: 3.170\n",
      "Epoch: 07 | Train Loss: 3.338\n",
      "Epoch: 07 | Train Loss: 3.558\n",
      "Epoch: 07 | Train Loss: 3.463\n",
      "Epoch: 07 | Train Loss: 3.340\n",
      "Epoch: 07 | Train Loss: 3.227\n",
      "Epoch: 07 | Train Loss: 3.377\n",
      "Epoch: 07 | Train Loss: 3.332\n",
      "Epoch: 07 | Train Loss: 3.353\n",
      "Epoch: 07 | Train Loss: 3.542\n",
      "Epoch: 07 | Train Loss: 3.636\n",
      "Epoch: 07 | Train Loss: 3.273\n",
      "Epoch: 07 | Train Loss: 3.282\n",
      "Epoch: 07 | Train Loss: 3.297\n",
      "Epoch: 07 | Train Loss: 3.294\n",
      "Epoch: 07 | Train Loss: 3.252\n",
      "Epoch: 07 | Train Loss: 3.298\n",
      "Epoch: 07 | Train Loss: 3.297\n",
      "Epoch: 07 | Train Loss: 3.435\n",
      "Epoch: 07 | Train Loss: 3.510\n",
      "Epoch: 07 | Train Loss: 3.351\n",
      "Epoch: 07 | Train Loss: 3.424\n",
      "Epoch: 07 | Train Loss: 3.360\n",
      "Epoch: 07 | Train Loss: 3.391\n",
      "Epoch: 07 | Train Loss: 3.337\n",
      "Epoch: 07 | Train Loss: 3.341\n",
      "Epoch: 07 | Train Loss: 3.479\n",
      "Epoch: 07 | Train Loss: 3.262\n",
      "Epoch: 07 | Train Loss: 3.499\n",
      "Epoch: 07 | Train Loss: 3.469\n",
      "Epoch: 07 | Train Loss: 3.479\n",
      "Epoch: 07 | Train Loss: 3.438\n",
      "Epoch: 07 | Train Loss: 3.417\n",
      "Epoch: 07 | Train Loss: 3.413\n",
      "Epoch: 07 | Train Loss: 3.364\n",
      "Epoch: 07 | Train Loss: 3.428\n",
      "Epoch: 07 | Train Loss: 3.597\n",
      "Epoch: 07 | Train Loss: 3.311\n",
      "Epoch: 07 | Train Loss: 3.478\n",
      "Epoch: 07 | Train Loss: 3.454\n",
      "Epoch: 07 | Train Loss: 3.344\n",
      "Epoch: 07 | Train Loss: 3.522\n",
      "Epoch: 07 | Train Loss: 3.481\n",
      "Epoch: 07 | Train Loss: 3.613\n",
      "Epoch: 07 | Train Loss: 3.453\n",
      "Epoch: 07 | Train Loss: 3.469\n",
      "Epoch: 07 | Train Loss: 3.441\n",
      "Epoch: 07 | Train Loss: 3.445\n",
      "Epoch: 07 | Train Loss: 3.412\n",
      "Epoch: 07 | Train Loss: 3.566\n",
      "Epoch: 07 | Train Loss: 3.337\n",
      "Epoch: 07 | Train Loss: 3.526\n",
      "Epoch: 07 | Train Loss: 3.345\n",
      "Epoch: 07 | Train Loss: 3.185\n",
      "Epoch: 08 | Train Loss: 2.793\n",
      "Epoch: 08 | Train Loss: 3.014\n",
      "Epoch: 08 | Train Loss: 3.013\n",
      "Epoch: 08 | Train Loss: 3.033\n",
      "Epoch: 08 | Train Loss: 2.867\n",
      "Epoch: 08 | Train Loss: 2.958\n",
      "Epoch: 08 | Train Loss: 2.967\n",
      "Epoch: 08 | Train Loss: 2.974\n",
      "Epoch: 08 | Train Loss: 2.806\n",
      "Epoch: 08 | Train Loss: 3.128\n",
      "Epoch: 08 | Train Loss: 2.927\n",
      "Epoch: 08 | Train Loss: 2.873\n",
      "Epoch: 08 | Train Loss: 3.000\n",
      "Epoch: 08 | Train Loss: 3.042\n",
      "Epoch: 08 | Train Loss: 2.914\n",
      "Epoch: 08 | Train Loss: 3.057\n",
      "Epoch: 08 | Train Loss: 3.033\n",
      "Epoch: 08 | Train Loss: 2.975\n",
      "Epoch: 08 | Train Loss: 3.180\n",
      "Epoch: 08 | Train Loss: 3.028\n",
      "Epoch: 08 | Train Loss: 3.075\n",
      "Epoch: 08 | Train Loss: 3.012\n",
      "Epoch: 08 | Train Loss: 3.040\n",
      "Epoch: 08 | Train Loss: 3.151\n",
      "Epoch: 08 | Train Loss: 2.921\n",
      "Epoch: 08 | Train Loss: 3.211\n",
      "Epoch: 08 | Train Loss: 3.087\n",
      "Epoch: 08 | Train Loss: 3.106\n",
      "Epoch: 08 | Train Loss: 3.085\n",
      "Epoch: 08 | Train Loss: 2.946\n",
      "Epoch: 08 | Train Loss: 2.940\n",
      "Epoch: 08 | Train Loss: 3.133\n",
      "Epoch: 08 | Train Loss: 3.108\n",
      "Epoch: 08 | Train Loss: 3.108\n",
      "Epoch: 08 | Train Loss: 2.913\n",
      "Epoch: 08 | Train Loss: 3.207\n",
      "Epoch: 08 | Train Loss: 3.112\n",
      "Epoch: 08 | Train Loss: 3.228\n",
      "Epoch: 08 | Train Loss: 3.123\n",
      "Epoch: 08 | Train Loss: 3.237\n",
      "Epoch: 08 | Train Loss: 3.011\n",
      "Epoch: 08 | Train Loss: 3.175\n",
      "Epoch: 08 | Train Loss: 3.281\n",
      "Epoch: 08 | Train Loss: 3.135\n",
      "Epoch: 08 | Train Loss: 3.085\n",
      "Epoch: 08 | Train Loss: 3.020\n",
      "Epoch: 08 | Train Loss: 3.076\n",
      "Epoch: 08 | Train Loss: 3.241\n",
      "Epoch: 08 | Train Loss: 3.176\n",
      "Epoch: 08 | Train Loss: 3.041\n",
      "Epoch: 08 | Train Loss: 3.149\n",
      "Epoch: 08 | Train Loss: 3.118\n",
      "Epoch: 08 | Train Loss: 3.249\n",
      "Epoch: 08 | Train Loss: 3.087\n",
      "Epoch: 08 | Train Loss: 2.921\n",
      "Epoch: 08 | Train Loss: 2.977\n",
      "Epoch: 08 | Train Loss: 3.093\n",
      "Epoch: 08 | Train Loss: 3.225\n",
      "Epoch: 08 | Train Loss: 3.293\n",
      "Epoch: 08 | Train Loss: 3.085\n",
      "Epoch: 08 | Train Loss: 3.160\n",
      "Epoch: 08 | Train Loss: 3.181\n",
      "Epoch: 08 | Train Loss: 3.169\n",
      "Epoch: 08 | Train Loss: 3.237\n",
      "Epoch: 08 | Train Loss: 3.112\n",
      "Epoch: 08 | Train Loss: 2.990\n",
      "Epoch: 08 | Train Loss: 3.215\n",
      "Epoch: 09 | Train Loss: 2.843\n",
      "Epoch: 09 | Train Loss: 2.670\n",
      "Epoch: 09 | Train Loss: 2.715\n",
      "Epoch: 09 | Train Loss: 2.712\n",
      "Epoch: 09 | Train Loss: 2.646\n",
      "Epoch: 09 | Train Loss: 2.556\n",
      "Epoch: 09 | Train Loss: 2.655\n",
      "Epoch: 09 | Train Loss: 2.626\n",
      "Epoch: 09 | Train Loss: 2.653\n",
      "Epoch: 09 | Train Loss: 2.679\n",
      "Epoch: 09 | Train Loss: 2.598\n",
      "Epoch: 09 | Train Loss: 2.552\n",
      "Epoch: 09 | Train Loss: 2.742\n",
      "Epoch: 09 | Train Loss: 2.690\n",
      "Epoch: 09 | Train Loss: 2.514\n",
      "Epoch: 09 | Train Loss: 2.624\n",
      "Epoch: 09 | Train Loss: 2.605\n",
      "Epoch: 09 | Train Loss: 2.681\n",
      "Epoch: 09 | Train Loss: 2.776\n",
      "Epoch: 09 | Train Loss: 2.839\n",
      "Epoch: 09 | Train Loss: 2.644\n",
      "Epoch: 09 | Train Loss: 2.812\n",
      "Epoch: 09 | Train Loss: 2.652\n",
      "Epoch: 09 | Train Loss: 2.552\n",
      "Epoch: 09 | Train Loss: 2.781\n",
      "Epoch: 09 | Train Loss: 2.770\n",
      "Epoch: 09 | Train Loss: 2.740\n",
      "Epoch: 09 | Train Loss: 2.747\n",
      "Epoch: 09 | Train Loss: 2.712\n",
      "Epoch: 09 | Train Loss: 2.692\n",
      "Epoch: 09 | Train Loss: 2.767\n",
      "Epoch: 09 | Train Loss: 2.456\n",
      "Epoch: 09 | Train Loss: 2.769\n",
      "Epoch: 09 | Train Loss: 2.756\n",
      "Epoch: 09 | Train Loss: 2.789\n",
      "Epoch: 09 | Train Loss: 2.773\n",
      "Epoch: 09 | Train Loss: 2.692\n",
      "Epoch: 09 | Train Loss: 2.820\n",
      "Epoch: 09 | Train Loss: 2.746\n",
      "Epoch: 09 | Train Loss: 2.928\n",
      "Epoch: 09 | Train Loss: 2.721\n",
      "Epoch: 09 | Train Loss: 2.720\n",
      "Epoch: 09 | Train Loss: 2.775\n",
      "Epoch: 09 | Train Loss: 2.662\n",
      "Epoch: 09 | Train Loss: 2.831\n",
      "Epoch: 09 | Train Loss: 2.762\n",
      "Epoch: 09 | Train Loss: 2.639\n",
      "Epoch: 09 | Train Loss: 2.829\n",
      "Epoch: 09 | Train Loss: 2.832\n",
      "Epoch: 09 | Train Loss: 2.793\n",
      "Epoch: 09 | Train Loss: 2.743\n",
      "Epoch: 09 | Train Loss: 2.860\n",
      "Epoch: 09 | Train Loss: 3.050\n",
      "Epoch: 09 | Train Loss: 2.767\n",
      "Epoch: 09 | Train Loss: 2.840\n",
      "Epoch: 09 | Train Loss: 2.898\n",
      "Epoch: 09 | Train Loss: 2.872\n",
      "Epoch: 09 | Train Loss: 2.802\n",
      "Epoch: 09 | Train Loss: 2.851\n",
      "Epoch: 09 | Train Loss: 2.936\n",
      "Epoch: 09 | Train Loss: 2.734\n",
      "Epoch: 09 | Train Loss: 2.889\n",
      "Epoch: 09 | Train Loss: 2.876\n",
      "Epoch: 09 | Train Loss: 2.790\n",
      "Epoch: 09 | Train Loss: 2.940\n",
      "Epoch: 09 | Train Loss: 2.959\n",
      "Epoch: 09 | Train Loss: 3.006\n",
      "Epoch: 10 | Train Loss: 2.168\n",
      "Epoch: 10 | Train Loss: 2.406\n",
      "Epoch: 10 | Train Loss: 2.295\n",
      "Epoch: 10 | Train Loss: 2.384\n",
      "Epoch: 10 | Train Loss: 2.330\n",
      "Epoch: 10 | Train Loss: 2.353\n",
      "Epoch: 10 | Train Loss: 2.415\n",
      "Epoch: 10 | Train Loss: 2.460\n",
      "Epoch: 10 | Train Loss: 2.441\n",
      "Epoch: 10 | Train Loss: 2.341\n",
      "Epoch: 10 | Train Loss: 2.298\n",
      "Epoch: 10 | Train Loss: 2.471\n",
      "Epoch: 10 | Train Loss: 2.486\n",
      "Epoch: 10 | Train Loss: 2.373\n",
      "Epoch: 10 | Train Loss: 2.486\n",
      "Epoch: 10 | Train Loss: 2.486\n",
      "Epoch: 10 | Train Loss: 2.378\n",
      "Epoch: 10 | Train Loss: 2.336\n",
      "Epoch: 10 | Train Loss: 2.373\n",
      "Epoch: 10 | Train Loss: 2.425\n",
      "Epoch: 10 | Train Loss: 2.187\n",
      "Epoch: 10 | Train Loss: 2.500\n",
      "Epoch: 10 | Train Loss: 2.551\n",
      "Epoch: 10 | Train Loss: 2.437\n",
      "Epoch: 10 | Train Loss: 2.391\n",
      "Epoch: 10 | Train Loss: 2.409\n",
      "Epoch: 10 | Train Loss: 2.413\n",
      "Epoch: 10 | Train Loss: 2.449\n",
      "Epoch: 10 | Train Loss: 2.431\n",
      "Epoch: 10 | Train Loss: 2.431\n",
      "Epoch: 10 | Train Loss: 2.503\n",
      "Epoch: 10 | Train Loss: 2.659\n",
      "Epoch: 10 | Train Loss: 2.526\n",
      "Epoch: 10 | Train Loss: 2.489\n",
      "Epoch: 10 | Train Loss: 2.496\n",
      "Epoch: 10 | Train Loss: 2.545\n",
      "Epoch: 10 | Train Loss: 2.536\n",
      "Epoch: 10 | Train Loss: 2.541\n",
      "Epoch: 10 | Train Loss: 2.414\n",
      "Epoch: 10 | Train Loss: 2.420\n",
      "Epoch: 10 | Train Loss: 2.513\n",
      "Epoch: 10 | Train Loss: 2.558\n",
      "Epoch: 10 | Train Loss: 2.445\n",
      "Epoch: 10 | Train Loss: 2.551\n",
      "Epoch: 10 | Train Loss: 2.512\n",
      "Epoch: 10 | Train Loss: 2.441\n",
      "Epoch: 10 | Train Loss: 2.614\n",
      "Epoch: 10 | Train Loss: 2.415\n",
      "Epoch: 10 | Train Loss: 2.500\n",
      "Epoch: 10 | Train Loss: 2.387\n",
      "Epoch: 10 | Train Loss: 2.367\n",
      "Epoch: 10 | Train Loss: 2.434\n",
      "Epoch: 10 | Train Loss: 2.485\n",
      "Epoch: 10 | Train Loss: 2.553\n",
      "Epoch: 10 | Train Loss: 2.545\n",
      "Epoch: 10 | Train Loss: 2.611\n",
      "Epoch: 10 | Train Loss: 2.417\n",
      "Epoch: 10 | Train Loss: 2.628\n",
      "Epoch: 10 | Train Loss: 2.641\n",
      "Epoch: 10 | Train Loss: 2.616\n",
      "Epoch: 10 | Train Loss: 2.448\n",
      "Epoch: 10 | Train Loss: 2.409\n",
      "Epoch: 10 | Train Loss: 2.599\n",
      "Epoch: 10 | Train Loss: 2.499\n",
      "Epoch: 10 | Train Loss: 2.638\n",
      "Epoch: 10 | Train Loss: 2.553\n",
      "Epoch: 10 | Train Loss: 2.750\n",
      "Epoch: 11 | Train Loss: 2.147\n",
      "Epoch: 11 | Train Loss: 2.298\n",
      "Epoch: 11 | Train Loss: 2.227\n",
      "Epoch: 11 | Train Loss: 2.162\n",
      "Epoch: 11 | Train Loss: 2.169\n",
      "Epoch: 11 | Train Loss: 2.177\n",
      "Epoch: 11 | Train Loss: 2.191\n",
      "Epoch: 11 | Train Loss: 2.172\n",
      "Epoch: 11 | Train Loss: 2.173\n",
      "Epoch: 11 | Train Loss: 2.265\n",
      "Epoch: 11 | Train Loss: 2.257\n",
      "Epoch: 11 | Train Loss: 2.101\n",
      "Epoch: 11 | Train Loss: 2.209\n",
      "Epoch: 11 | Train Loss: 2.042\n",
      "Epoch: 11 | Train Loss: 2.051\n",
      "Epoch: 11 | Train Loss: 2.172\n",
      "Epoch: 11 | Train Loss: 2.190\n",
      "Epoch: 11 | Train Loss: 2.183\n",
      "Epoch: 11 | Train Loss: 2.295\n",
      "Epoch: 11 | Train Loss: 2.226\n",
      "Epoch: 11 | Train Loss: 2.213\n",
      "Epoch: 11 | Train Loss: 2.180\n",
      "Epoch: 11 | Train Loss: 2.147\n",
      "Epoch: 11 | Train Loss: 2.201\n",
      "Epoch: 11 | Train Loss: 2.259\n",
      "Epoch: 11 | Train Loss: 2.079\n",
      "Epoch: 11 | Train Loss: 2.182\n",
      "Epoch: 11 | Train Loss: 2.167\n",
      "Epoch: 11 | Train Loss: 2.050\n",
      "Epoch: 11 | Train Loss: 2.154\n",
      "Epoch: 11 | Train Loss: 2.237\n",
      "Epoch: 11 | Train Loss: 2.191\n",
      "Epoch: 11 | Train Loss: 2.167\n",
      "Epoch: 11 | Train Loss: 2.283\n",
      "Epoch: 11 | Train Loss: 2.339\n",
      "Epoch: 11 | Train Loss: 2.311\n",
      "Epoch: 11 | Train Loss: 2.266\n",
      "Epoch: 11 | Train Loss: 2.069\n",
      "Epoch: 11 | Train Loss: 2.294\n",
      "Epoch: 11 | Train Loss: 2.078\n",
      "Epoch: 11 | Train Loss: 2.370\n",
      "Epoch: 11 | Train Loss: 2.287\n",
      "Epoch: 11 | Train Loss: 2.096\n",
      "Epoch: 11 | Train Loss: 1.835\n",
      "Epoch: 11 | Train Loss: 2.179\n",
      "Epoch: 11 | Train Loss: 2.178\n",
      "Epoch: 11 | Train Loss: 2.200\n",
      "Epoch: 11 | Train Loss: 2.084\n",
      "Epoch: 11 | Train Loss: 2.124\n",
      "Epoch: 11 | Train Loss: 2.298\n",
      "Epoch: 11 | Train Loss: 2.189\n",
      "Epoch: 11 | Train Loss: 2.140\n",
      "Epoch: 11 | Train Loss: 2.147\n",
      "Epoch: 11 | Train Loss: 2.253\n",
      "Epoch: 11 | Train Loss: 2.323\n",
      "Epoch: 11 | Train Loss: 2.451\n",
      "Epoch: 11 | Train Loss: 2.177\n",
      "Epoch: 11 | Train Loss: 2.226\n",
      "Epoch: 11 | Train Loss: 2.366\n",
      "Epoch: 11 | Train Loss: 2.239\n",
      "Epoch: 11 | Train Loss: 2.356\n",
      "Epoch: 11 | Train Loss: 2.153\n",
      "Epoch: 11 | Train Loss: 2.351\n",
      "Epoch: 11 | Train Loss: 2.185\n",
      "Epoch: 11 | Train Loss: 2.218\n",
      "Epoch: 11 | Train Loss: 2.357\n",
      "Epoch: 11 | Train Loss: 2.379\n",
      "Epoch: 12 | Train Loss: 1.846\n",
      "Epoch: 12 | Train Loss: 1.804\n",
      "Epoch: 12 | Train Loss: 1.828\n",
      "Epoch: 12 | Train Loss: 1.908\n",
      "Epoch: 12 | Train Loss: 1.729\n",
      "Epoch: 12 | Train Loss: 1.887\n",
      "Epoch: 12 | Train Loss: 1.924\n",
      "Epoch: 12 | Train Loss: 1.746\n",
      "Epoch: 12 | Train Loss: 1.910\n",
      "Epoch: 12 | Train Loss: 2.023\n",
      "Epoch: 12 | Train Loss: 1.760\n",
      "Epoch: 12 | Train Loss: 1.968\n",
      "Epoch: 12 | Train Loss: 1.823\n",
      "Epoch: 12 | Train Loss: 1.916\n",
      "Epoch: 12 | Train Loss: 1.972\n",
      "Epoch: 12 | Train Loss: 1.854\n",
      "Epoch: 12 | Train Loss: 1.880\n",
      "Epoch: 12 | Train Loss: 2.013\n",
      "Epoch: 12 | Train Loss: 2.125\n",
      "Epoch: 12 | Train Loss: 1.837\n",
      "Epoch: 12 | Train Loss: 1.956\n",
      "Epoch: 12 | Train Loss: 1.953\n",
      "Epoch: 12 | Train Loss: 1.979\n",
      "Epoch: 12 | Train Loss: 1.978\n",
      "Epoch: 12 | Train Loss: 1.806\n",
      "Epoch: 12 | Train Loss: 1.738\n",
      "Epoch: 12 | Train Loss: 1.991\n",
      "Epoch: 12 | Train Loss: 1.993\n",
      "Epoch: 12 | Train Loss: 2.011\n",
      "Epoch: 12 | Train Loss: 2.007\n",
      "Epoch: 12 | Train Loss: 2.031\n",
      "Epoch: 12 | Train Loss: 1.929\n",
      "Epoch: 12 | Train Loss: 1.993\n",
      "Epoch: 12 | Train Loss: 2.067\n",
      "Epoch: 12 | Train Loss: 1.907\n",
      "Epoch: 12 | Train Loss: 1.961\n",
      "Epoch: 12 | Train Loss: 2.087\n",
      "Epoch: 12 | Train Loss: 2.022\n",
      "Epoch: 12 | Train Loss: 1.986\n",
      "Epoch: 12 | Train Loss: 1.901\n",
      "Epoch: 12 | Train Loss: 1.838\n",
      "Epoch: 12 | Train Loss: 2.075\n",
      "Epoch: 12 | Train Loss: 2.014\n",
      "Epoch: 12 | Train Loss: 1.868\n",
      "Epoch: 12 | Train Loss: 1.996\n",
      "Epoch: 12 | Train Loss: 1.938\n",
      "Epoch: 12 | Train Loss: 1.882\n",
      "Epoch: 12 | Train Loss: 1.945\n",
      "Epoch: 12 | Train Loss: 1.868\n",
      "Epoch: 12 | Train Loss: 2.077\n",
      "Epoch: 12 | Train Loss: 2.098\n",
      "Epoch: 12 | Train Loss: 1.957\n",
      "Epoch: 12 | Train Loss: 1.945\n",
      "Epoch: 12 | Train Loss: 2.032\n",
      "Epoch: 12 | Train Loss: 2.089\n",
      "Epoch: 12 | Train Loss: 2.019\n",
      "Epoch: 12 | Train Loss: 1.952\n",
      "Epoch: 12 | Train Loss: 1.989\n",
      "Epoch: 12 | Train Loss: 2.051\n",
      "Epoch: 12 | Train Loss: 2.087\n",
      "Epoch: 12 | Train Loss: 2.000\n",
      "Epoch: 12 | Train Loss: 2.192\n",
      "Epoch: 12 | Train Loss: 1.974\n",
      "Epoch: 12 | Train Loss: 2.112\n",
      "Epoch: 12 | Train Loss: 1.988\n",
      "Epoch: 12 | Train Loss: 1.981\n",
      "Epoch: 12 | Train Loss: 2.085\n",
      "Epoch: 13 | Train Loss: 1.639\n",
      "Epoch: 13 | Train Loss: 1.660\n",
      "Epoch: 13 | Train Loss: 1.740\n",
      "Epoch: 13 | Train Loss: 1.780\n",
      "Epoch: 13 | Train Loss: 1.613\n",
      "Epoch: 13 | Train Loss: 1.644\n",
      "Epoch: 13 | Train Loss: 1.487\n",
      "Epoch: 13 | Train Loss: 1.638\n",
      "Epoch: 13 | Train Loss: 1.679\n",
      "Epoch: 13 | Train Loss: 1.616\n",
      "Epoch: 13 | Train Loss: 1.526\n",
      "Epoch: 13 | Train Loss: 1.791\n",
      "Epoch: 13 | Train Loss: 1.636\n",
      "Epoch: 13 | Train Loss: 1.725\n",
      "Epoch: 13 | Train Loss: 1.786\n",
      "Epoch: 13 | Train Loss: 1.618\n",
      "Epoch: 13 | Train Loss: 1.577\n",
      "Epoch: 13 | Train Loss: 1.750\n",
      "Epoch: 13 | Train Loss: 1.621\n",
      "Epoch: 13 | Train Loss: 1.770\n",
      "Epoch: 13 | Train Loss: 1.707\n",
      "Epoch: 13 | Train Loss: 1.609\n",
      "Epoch: 13 | Train Loss: 1.764\n",
      "Epoch: 13 | Train Loss: 1.686\n",
      "Epoch: 13 | Train Loss: 1.767\n",
      "Epoch: 13 | Train Loss: 1.551\n",
      "Epoch: 13 | Train Loss: 1.795\n",
      "Epoch: 13 | Train Loss: 1.616\n",
      "Epoch: 13 | Train Loss: 1.794\n",
      "Epoch: 13 | Train Loss: 1.727\n",
      "Epoch: 13 | Train Loss: 1.751\n",
      "Epoch: 13 | Train Loss: 1.788\n",
      "Epoch: 13 | Train Loss: 1.795\n",
      "Epoch: 13 | Train Loss: 1.729\n",
      "Epoch: 13 | Train Loss: 1.735\n",
      "Epoch: 13 | Train Loss: 1.707\n",
      "Epoch: 13 | Train Loss: 1.702\n",
      "Epoch: 13 | Train Loss: 1.807\n",
      "Epoch: 13 | Train Loss: 1.755\n",
      "Epoch: 13 | Train Loss: 1.856\n",
      "Epoch: 13 | Train Loss: 1.758\n",
      "Epoch: 13 | Train Loss: 1.692\n",
      "Epoch: 13 | Train Loss: 1.802\n",
      "Epoch: 13 | Train Loss: 1.744\n",
      "Epoch: 13 | Train Loss: 1.796\n",
      "Epoch: 13 | Train Loss: 1.746\n",
      "Epoch: 13 | Train Loss: 1.789\n",
      "Epoch: 13 | Train Loss: 1.745\n",
      "Epoch: 13 | Train Loss: 1.820\n",
      "Epoch: 13 | Train Loss: 1.747\n",
      "Epoch: 13 | Train Loss: 1.781\n",
      "Epoch: 13 | Train Loss: 1.708\n",
      "Epoch: 13 | Train Loss: 1.753\n",
      "Epoch: 13 | Train Loss: 1.962\n",
      "Epoch: 13 | Train Loss: 1.938\n",
      "Epoch: 13 | Train Loss: 1.649\n",
      "Epoch: 13 | Train Loss: 1.750\n",
      "Epoch: 13 | Train Loss: 1.802\n",
      "Epoch: 13 | Train Loss: 1.835\n",
      "Epoch: 13 | Train Loss: 1.808\n",
      "Epoch: 13 | Train Loss: 1.867\n",
      "Epoch: 13 | Train Loss: 1.799\n",
      "Epoch: 13 | Train Loss: 2.050\n",
      "Epoch: 13 | Train Loss: 1.891\n",
      "Epoch: 13 | Train Loss: 1.874\n",
      "Epoch: 13 | Train Loss: 1.732\n",
      "Epoch: 13 | Train Loss: 1.882\n",
      "Epoch: 14 | Train Loss: 1.369\n",
      "Epoch: 14 | Train Loss: 1.497\n",
      "Epoch: 14 | Train Loss: 1.502\n",
      "Epoch: 14 | Train Loss: 1.410\n",
      "Epoch: 14 | Train Loss: 1.440\n",
      "Epoch: 14 | Train Loss: 1.500\n",
      "Epoch: 14 | Train Loss: 1.401\n",
      "Epoch: 14 | Train Loss: 1.348\n",
      "Epoch: 14 | Train Loss: 1.428\n",
      "Epoch: 14 | Train Loss: 1.408\n",
      "Epoch: 14 | Train Loss: 1.561\n",
      "Epoch: 14 | Train Loss: 1.333\n",
      "Epoch: 14 | Train Loss: 1.544\n",
      "Epoch: 14 | Train Loss: 1.491\n",
      "Epoch: 14 | Train Loss: 1.546\n",
      "Epoch: 14 | Train Loss: 1.288\n",
      "Epoch: 14 | Train Loss: 1.572\n",
      "Epoch: 14 | Train Loss: 1.376\n",
      "Epoch: 14 | Train Loss: 1.615\n",
      "Epoch: 14 | Train Loss: 1.587\n",
      "Epoch: 14 | Train Loss: 1.494\n",
      "Epoch: 14 | Train Loss: 1.606\n",
      "Epoch: 14 | Train Loss: 1.488\n",
      "Epoch: 14 | Train Loss: 1.523\n",
      "Epoch: 14 | Train Loss: 1.424\n",
      "Epoch: 14 | Train Loss: 1.510\n",
      "Epoch: 14 | Train Loss: 1.527\n",
      "Epoch: 14 | Train Loss: 1.463\n",
      "Epoch: 14 | Train Loss: 1.536\n",
      "Epoch: 14 | Train Loss: 1.564\n",
      "Epoch: 14 | Train Loss: 1.524\n",
      "Epoch: 14 | Train Loss: 1.552\n",
      "Epoch: 14 | Train Loss: 1.517\n",
      "Epoch: 14 | Train Loss: 1.541\n",
      "Epoch: 14 | Train Loss: 1.521\n",
      "Epoch: 14 | Train Loss: 1.533\n",
      "Epoch: 14 | Train Loss: 1.594\n",
      "Epoch: 14 | Train Loss: 1.626\n",
      "Epoch: 14 | Train Loss: 1.499\n",
      "Epoch: 14 | Train Loss: 1.544\n",
      "Epoch: 14 | Train Loss: 1.609\n",
      "Epoch: 14 | Train Loss: 1.549\n",
      "Epoch: 14 | Train Loss: 1.583\n",
      "Epoch: 14 | Train Loss: 1.584\n",
      "Epoch: 14 | Train Loss: 1.520\n",
      "Epoch: 14 | Train Loss: 1.506\n",
      "Epoch: 14 | Train Loss: 1.674\n",
      "Epoch: 14 | Train Loss: 1.666\n",
      "Epoch: 14 | Train Loss: 1.539\n",
      "Epoch: 14 | Train Loss: 1.627\n",
      "Epoch: 14 | Train Loss: 1.550\n",
      "Epoch: 14 | Train Loss: 1.608\n",
      "Epoch: 14 | Train Loss: 1.563\n",
      "Epoch: 14 | Train Loss: 1.709\n",
      "Epoch: 14 | Train Loss: 1.546\n",
      "Epoch: 14 | Train Loss: 1.642\n",
      "Epoch: 14 | Train Loss: 1.550\n",
      "Epoch: 14 | Train Loss: 1.660\n",
      "Epoch: 14 | Train Loss: 1.524\n",
      "Epoch: 14 | Train Loss: 1.696\n",
      "Epoch: 14 | Train Loss: 1.631\n",
      "Epoch: 14 | Train Loss: 1.639\n",
      "Epoch: 14 | Train Loss: 1.590\n",
      "Epoch: 14 | Train Loss: 1.690\n",
      "Epoch: 14 | Train Loss: 1.548\n",
      "Epoch: 14 | Train Loss: 1.595\n",
      "Epoch: 14 | Train Loss: 1.634\n",
      "Epoch: 15 | Train Loss: 1.228\n",
      "Epoch: 15 | Train Loss: 1.189\n",
      "Epoch: 15 | Train Loss: 1.287\n",
      "Epoch: 15 | Train Loss: 1.270\n",
      "Epoch: 15 | Train Loss: 1.287\n",
      "Epoch: 15 | Train Loss: 1.238\n",
      "Epoch: 15 | Train Loss: 1.290\n",
      "Epoch: 15 | Train Loss: 1.216\n",
      "Epoch: 15 | Train Loss: 1.311\n",
      "Epoch: 15 | Train Loss: 1.147\n",
      "Epoch: 15 | Train Loss: 1.371\n",
      "Epoch: 15 | Train Loss: 1.387\n",
      "Epoch: 15 | Train Loss: 1.233\n",
      "Epoch: 15 | Train Loss: 1.284\n",
      "Epoch: 15 | Train Loss: 1.192\n",
      "Epoch: 15 | Train Loss: 1.327\n",
      "Epoch: 15 | Train Loss: 1.251\n",
      "Epoch: 15 | Train Loss: 1.311\n",
      "Epoch: 15 | Train Loss: 1.339\n",
      "Epoch: 15 | Train Loss: 1.295\n",
      "Epoch: 15 | Train Loss: 1.279\n",
      "Epoch: 15 | Train Loss: 1.346\n",
      "Epoch: 15 | Train Loss: 1.292\n",
      "Epoch: 15 | Train Loss: 1.353\n",
      "Epoch: 15 | Train Loss: 1.481\n",
      "Epoch: 15 | Train Loss: 1.405\n",
      "Epoch: 15 | Train Loss: 1.394\n",
      "Epoch: 15 | Train Loss: 1.404\n",
      "Epoch: 15 | Train Loss: 1.182\n",
      "Epoch: 15 | Train Loss: 1.371\n",
      "Epoch: 15 | Train Loss: 1.442\n",
      "Epoch: 15 | Train Loss: 1.291\n",
      "Epoch: 15 | Train Loss: 1.271\n",
      "Epoch: 15 | Train Loss: 1.175\n",
      "Epoch: 15 | Train Loss: 1.398\n",
      "Epoch: 15 | Train Loss: 1.416\n",
      "Epoch: 15 | Train Loss: 1.404\n",
      "Epoch: 15 | Train Loss: 1.291\n",
      "Epoch: 15 | Train Loss: 1.279\n",
      "Epoch: 15 | Train Loss: 1.356\n",
      "Epoch: 15 | Train Loss: 1.471\n",
      "Epoch: 15 | Train Loss: 1.440\n",
      "Epoch: 15 | Train Loss: 1.293\n",
      "Epoch: 15 | Train Loss: 1.492\n",
      "Epoch: 15 | Train Loss: 1.326\n",
      "Epoch: 15 | Train Loss: 1.358\n",
      "Epoch: 15 | Train Loss: 1.438\n",
      "Epoch: 15 | Train Loss: 1.376\n",
      "Epoch: 15 | Train Loss: 1.438\n",
      "Epoch: 15 | Train Loss: 1.453\n",
      "Epoch: 15 | Train Loss: 1.520\n",
      "Epoch: 15 | Train Loss: 1.444\n",
      "Epoch: 15 | Train Loss: 1.446\n",
      "Epoch: 15 | Train Loss: 1.287\n",
      "Epoch: 15 | Train Loss: 1.338\n",
      "Epoch: 15 | Train Loss: 1.312\n",
      "Epoch: 15 | Train Loss: 1.421\n",
      "Epoch: 15 | Train Loss: 1.360\n",
      "Epoch: 15 | Train Loss: 1.442\n",
      "Epoch: 15 | Train Loss: 1.473\n",
      "Epoch: 15 | Train Loss: 1.507\n",
      "Epoch: 15 | Train Loss: 1.327\n",
      "Epoch: 15 | Train Loss: 1.353\n",
      "Epoch: 15 | Train Loss: 1.566\n",
      "Epoch: 15 | Train Loss: 1.428\n",
      "Epoch: 15 | Train Loss: 1.429\n",
      "Epoch: 15 | Train Loss: 1.451\n",
      "Epoch: 16 | Train Loss: 0.970\n",
      "Epoch: 16 | Train Loss: 1.098\n",
      "Epoch: 16 | Train Loss: 1.098\n",
      "Epoch: 16 | Train Loss: 1.084\n",
      "Epoch: 16 | Train Loss: 1.108\n",
      "Epoch: 16 | Train Loss: 1.099\n",
      "Epoch: 16 | Train Loss: 1.071\n",
      "Epoch: 16 | Train Loss: 1.151\n",
      "Epoch: 16 | Train Loss: 1.128\n",
      "Epoch: 16 | Train Loss: 1.235\n",
      "Epoch: 16 | Train Loss: 1.325\n",
      "Epoch: 16 | Train Loss: 1.113\n",
      "Epoch: 16 | Train Loss: 1.079\n",
      "Epoch: 16 | Train Loss: 1.170\n",
      "Epoch: 16 | Train Loss: 1.137\n",
      "Epoch: 16 | Train Loss: 1.178\n",
      "Epoch: 16 | Train Loss: 1.226\n",
      "Epoch: 16 | Train Loss: 1.176\n",
      "Epoch: 16 | Train Loss: 1.153\n",
      "Epoch: 16 | Train Loss: 1.230\n",
      "Epoch: 16 | Train Loss: 1.260\n",
      "Epoch: 16 | Train Loss: 1.196\n",
      "Epoch: 16 | Train Loss: 1.102\n",
      "Epoch: 16 | Train Loss: 1.143\n",
      "Epoch: 16 | Train Loss: 1.112\n",
      "Epoch: 16 | Train Loss: 1.199\n",
      "Epoch: 16 | Train Loss: 1.214\n",
      "Epoch: 16 | Train Loss: 1.106\n",
      "Epoch: 16 | Train Loss: 1.237\n",
      "Epoch: 16 | Train Loss: 1.126\n",
      "Epoch: 16 | Train Loss: 1.198\n",
      "Epoch: 16 | Train Loss: 1.216\n",
      "Epoch: 16 | Train Loss: 1.185\n",
      "Epoch: 16 | Train Loss: 1.308\n",
      "Epoch: 16 | Train Loss: 1.209\n",
      "Epoch: 16 | Train Loss: 1.137\n",
      "Epoch: 16 | Train Loss: 1.235\n",
      "Epoch: 16 | Train Loss: 1.328\n",
      "Epoch: 16 | Train Loss: 1.338\n",
      "Epoch: 16 | Train Loss: 1.218\n",
      "Epoch: 16 | Train Loss: 1.257\n",
      "Epoch: 16 | Train Loss: 1.101\n",
      "Epoch: 16 | Train Loss: 1.211\n",
      "Epoch: 16 | Train Loss: 1.262\n",
      "Epoch: 16 | Train Loss: 1.257\n",
      "Epoch: 16 | Train Loss: 1.209\n",
      "Epoch: 16 | Train Loss: 1.172\n",
      "Epoch: 16 | Train Loss: 1.355\n",
      "Epoch: 16 | Train Loss: 1.236\n",
      "Epoch: 16 | Train Loss: 1.125\n",
      "Epoch: 16 | Train Loss: 1.142\n",
      "Epoch: 16 | Train Loss: 1.262\n",
      "Epoch: 16 | Train Loss: 1.152\n",
      "Epoch: 16 | Train Loss: 1.232\n",
      "Epoch: 16 | Train Loss: 1.140\n",
      "Epoch: 16 | Train Loss: 1.310\n",
      "Epoch: 16 | Train Loss: 1.218\n",
      "Epoch: 16 | Train Loss: 1.228\n",
      "Epoch: 16 | Train Loss: 1.262\n",
      "Epoch: 16 | Train Loss: 1.292\n",
      "Epoch: 16 | Train Loss: 1.241\n",
      "Epoch: 16 | Train Loss: 1.236\n",
      "Epoch: 16 | Train Loss: 1.124\n",
      "Epoch: 16 | Train Loss: 1.322\n",
      "Epoch: 16 | Train Loss: 1.361\n",
      "Epoch: 16 | Train Loss: 1.269\n",
      "Epoch: 16 | Train Loss: 1.276\n",
      "Epoch: 17 | Train Loss: 1.068\n",
      "Epoch: 17 | Train Loss: 1.022\n",
      "Epoch: 17 | Train Loss: 0.944\n",
      "Epoch: 17 | Train Loss: 0.862\n",
      "Epoch: 17 | Train Loss: 0.903\n",
      "Epoch: 17 | Train Loss: 0.895\n",
      "Epoch: 17 | Train Loss: 0.949\n",
      "Epoch: 17 | Train Loss: 1.053\n",
      "Epoch: 17 | Train Loss: 0.919\n",
      "Epoch: 17 | Train Loss: 0.932\n",
      "Epoch: 17 | Train Loss: 1.056\n",
      "Epoch: 17 | Train Loss: 0.997\n",
      "Epoch: 17 | Train Loss: 0.966\n",
      "Epoch: 17 | Train Loss: 0.984\n",
      "Epoch: 17 | Train Loss: 0.969\n",
      "Epoch: 17 | Train Loss: 0.950\n",
      "Epoch: 17 | Train Loss: 1.023\n",
      "Epoch: 17 | Train Loss: 0.996\n",
      "Epoch: 17 | Train Loss: 1.055\n",
      "Epoch: 17 | Train Loss: 1.069\n",
      "Epoch: 17 | Train Loss: 1.052\n",
      "Epoch: 17 | Train Loss: 1.009\n",
      "Epoch: 17 | Train Loss: 0.974\n",
      "Epoch: 17 | Train Loss: 1.044\n",
      "Epoch: 17 | Train Loss: 1.022\n",
      "Epoch: 17 | Train Loss: 1.046\n",
      "Epoch: 17 | Train Loss: 0.988\n",
      "Epoch: 17 | Train Loss: 0.989\n",
      "Epoch: 17 | Train Loss: 1.044\n",
      "Epoch: 17 | Train Loss: 1.077\n",
      "Epoch: 17 | Train Loss: 1.019\n",
      "Epoch: 17 | Train Loss: 1.185\n",
      "Epoch: 17 | Train Loss: 1.010\n",
      "Epoch: 17 | Train Loss: 1.014\n",
      "Epoch: 17 | Train Loss: 1.180\n",
      "Epoch: 17 | Train Loss: 1.080\n",
      "Epoch: 17 | Train Loss: 1.164\n",
      "Epoch: 17 | Train Loss: 1.036\n",
      "Epoch: 17 | Train Loss: 1.117\n",
      "Epoch: 17 | Train Loss: 1.187\n",
      "Epoch: 17 | Train Loss: 1.039\n",
      "Epoch: 17 | Train Loss: 1.010\n",
      "Epoch: 17 | Train Loss: 0.980\n",
      "Epoch: 17 | Train Loss: 1.057\n",
      "Epoch: 17 | Train Loss: 0.996\n",
      "Epoch: 17 | Train Loss: 1.099\n",
      "Epoch: 17 | Train Loss: 1.167\n",
      "Epoch: 17 | Train Loss: 1.133\n",
      "Epoch: 17 | Train Loss: 1.133\n",
      "Epoch: 17 | Train Loss: 1.050\n",
      "Epoch: 17 | Train Loss: 1.189\n",
      "Epoch: 17 | Train Loss: 1.111\n",
      "Epoch: 17 | Train Loss: 0.987\n",
      "Epoch: 17 | Train Loss: 1.083\n",
      "Epoch: 17 | Train Loss: 1.174\n",
      "Epoch: 17 | Train Loss: 1.048\n",
      "Epoch: 17 | Train Loss: 1.046\n",
      "Epoch: 17 | Train Loss: 1.059\n",
      "Epoch: 17 | Train Loss: 1.118\n",
      "Epoch: 17 | Train Loss: 1.093\n",
      "Epoch: 17 | Train Loss: 1.059\n",
      "Epoch: 17 | Train Loss: 1.052\n",
      "Epoch: 17 | Train Loss: 1.021\n",
      "Epoch: 17 | Train Loss: 1.025\n",
      "Epoch: 17 | Train Loss: 1.178\n",
      "Epoch: 17 | Train Loss: 1.100\n",
      "Epoch: 17 | Train Loss: 1.100\n",
      "Epoch: 18 | Train Loss: 0.795\n",
      "Epoch: 18 | Train Loss: 0.777\n",
      "Epoch: 18 | Train Loss: 0.956\n",
      "Epoch: 18 | Train Loss: 0.830\n",
      "Epoch: 18 | Train Loss: 0.806\n",
      "Epoch: 18 | Train Loss: 0.755\n",
      "Epoch: 18 | Train Loss: 0.754\n",
      "Epoch: 18 | Train Loss: 0.990\n",
      "Epoch: 18 | Train Loss: 0.816\n",
      "Epoch: 18 | Train Loss: 0.791\n",
      "Epoch: 18 | Train Loss: 0.949\n",
      "Epoch: 18 | Train Loss: 0.882\n",
      "Epoch: 18 | Train Loss: 0.841\n",
      "Epoch: 18 | Train Loss: 0.778\n",
      "Epoch: 18 | Train Loss: 0.822\n",
      "Epoch: 18 | Train Loss: 0.889\n",
      "Epoch: 18 | Train Loss: 0.861\n",
      "Epoch: 18 | Train Loss: 0.965\n",
      "Epoch: 18 | Train Loss: 0.922\n",
      "Epoch: 18 | Train Loss: 0.904\n",
      "Epoch: 18 | Train Loss: 0.903\n",
      "Epoch: 18 | Train Loss: 0.987\n",
      "Epoch: 18 | Train Loss: 0.954\n",
      "Epoch: 18 | Train Loss: 0.942\n",
      "Epoch: 18 | Train Loss: 0.909\n",
      "Epoch: 18 | Train Loss: 0.938\n",
      "Epoch: 18 | Train Loss: 0.873\n",
      "Epoch: 18 | Train Loss: 0.904\n",
      "Epoch: 18 | Train Loss: 0.874\n",
      "Epoch: 18 | Train Loss: 0.888\n",
      "Epoch: 18 | Train Loss: 0.907\n",
      "Epoch: 18 | Train Loss: 0.991\n",
      "Epoch: 18 | Train Loss: 0.949\n",
      "Epoch: 18 | Train Loss: 0.968\n",
      "Epoch: 18 | Train Loss: 0.890\n",
      "Epoch: 18 | Train Loss: 0.939\n",
      "Epoch: 18 | Train Loss: 0.879\n",
      "Epoch: 18 | Train Loss: 0.892\n",
      "Epoch: 18 | Train Loss: 0.894\n",
      "Epoch: 18 | Train Loss: 0.909\n",
      "Epoch: 18 | Train Loss: 0.891\n",
      "Epoch: 18 | Train Loss: 0.918\n",
      "Epoch: 18 | Train Loss: 0.928\n",
      "Epoch: 18 | Train Loss: 0.984\n",
      "Epoch: 18 | Train Loss: 0.948\n",
      "Epoch: 18 | Train Loss: 0.967\n",
      "Epoch: 18 | Train Loss: 0.909\n",
      "Epoch: 18 | Train Loss: 1.000\n",
      "Epoch: 18 | Train Loss: 0.974\n",
      "Epoch: 18 | Train Loss: 0.910\n",
      "Epoch: 18 | Train Loss: 0.979\n",
      "Epoch: 18 | Train Loss: 0.965\n",
      "Epoch: 18 | Train Loss: 0.953\n",
      "Epoch: 18 | Train Loss: 0.938\n",
      "Epoch: 18 | Train Loss: 0.936\n",
      "Epoch: 18 | Train Loss: 0.920\n",
      "Epoch: 18 | Train Loss: 0.955\n",
      "Epoch: 18 | Train Loss: 0.919\n",
      "Epoch: 18 | Train Loss: 0.982\n",
      "Epoch: 18 | Train Loss: 0.984\n",
      "Epoch: 18 | Train Loss: 0.942\n",
      "Epoch: 18 | Train Loss: 0.949\n",
      "Epoch: 18 | Train Loss: 0.939\n",
      "Epoch: 18 | Train Loss: 1.065\n",
      "Epoch: 18 | Train Loss: 0.930\n",
      "Epoch: 18 | Train Loss: 0.935\n",
      "Epoch: 18 | Train Loss: 1.130\n",
      "Epoch: 19 | Train Loss: 0.767\n",
      "Epoch: 19 | Train Loss: 0.722\n",
      "Epoch: 19 | Train Loss: 0.714\n",
      "Epoch: 19 | Train Loss: 0.663\n",
      "Epoch: 19 | Train Loss: 0.672\n",
      "Epoch: 19 | Train Loss: 0.713\n",
      "Epoch: 19 | Train Loss: 0.709\n",
      "Epoch: 19 | Train Loss: 0.753\n",
      "Epoch: 19 | Train Loss: 0.705\n",
      "Epoch: 19 | Train Loss: 0.720\n",
      "Epoch: 19 | Train Loss: 0.715\n",
      "Epoch: 19 | Train Loss: 0.759\n",
      "Epoch: 19 | Train Loss: 0.735\n",
      "Epoch: 19 | Train Loss: 0.846\n",
      "Epoch: 19 | Train Loss: 0.753\n",
      "Epoch: 19 | Train Loss: 0.822\n",
      "Epoch: 19 | Train Loss: 0.760\n",
      "Epoch: 19 | Train Loss: 0.788\n",
      "Epoch: 19 | Train Loss: 0.800\n",
      "Epoch: 19 | Train Loss: 0.785\n",
      "Epoch: 19 | Train Loss: 0.748\n",
      "Epoch: 19 | Train Loss: 0.803\n",
      "Epoch: 19 | Train Loss: 0.830\n",
      "Epoch: 19 | Train Loss: 0.803\n",
      "Epoch: 19 | Train Loss: 0.768\n",
      "Epoch: 19 | Train Loss: 0.856\n",
      "Epoch: 19 | Train Loss: 0.820\n",
      "Epoch: 19 | Train Loss: 0.717\n",
      "Epoch: 19 | Train Loss: 0.783\n",
      "Epoch: 19 | Train Loss: 0.815\n",
      "Epoch: 19 | Train Loss: 0.758\n",
      "Epoch: 19 | Train Loss: 0.855\n",
      "Epoch: 19 | Train Loss: 0.812\n",
      "Epoch: 19 | Train Loss: 0.798\n",
      "Epoch: 19 | Train Loss: 0.767\n",
      "Epoch: 19 | Train Loss: 0.817\n",
      "Epoch: 19 | Train Loss: 0.782\n",
      "Epoch: 19 | Train Loss: 0.772\n",
      "Epoch: 19 | Train Loss: 0.849\n",
      "Epoch: 19 | Train Loss: 0.798\n",
      "Epoch: 19 | Train Loss: 0.790\n",
      "Epoch: 19 | Train Loss: 0.835\n",
      "Epoch: 19 | Train Loss: 0.778\n",
      "Epoch: 19 | Train Loss: 0.761\n",
      "Epoch: 19 | Train Loss: 0.864\n",
      "Epoch: 19 | Train Loss: 0.770\n",
      "Epoch: 19 | Train Loss: 0.882\n",
      "Epoch: 19 | Train Loss: 0.811\n",
      "Epoch: 19 | Train Loss: 0.735\n",
      "Epoch: 19 | Train Loss: 0.871\n",
      "Epoch: 19 | Train Loss: 0.828\n",
      "Epoch: 19 | Train Loss: 0.866\n",
      "Epoch: 19 | Train Loss: 0.815\n",
      "Epoch: 19 | Train Loss: 0.836\n",
      "Epoch: 19 | Train Loss: 0.882\n",
      "Epoch: 19 | Train Loss: 0.859\n",
      "Epoch: 19 | Train Loss: 0.851\n",
      "Epoch: 19 | Train Loss: 0.840\n",
      "Epoch: 19 | Train Loss: 0.853\n",
      "Epoch: 19 | Train Loss: 0.796\n",
      "Epoch: 19 | Train Loss: 0.796\n",
      "Epoch: 19 | Train Loss: 0.796\n",
      "Epoch: 19 | Train Loss: 0.848\n",
      "Epoch: 19 | Train Loss: 0.900\n",
      "Epoch: 19 | Train Loss: 0.912\n",
      "Epoch: 19 | Train Loss: 0.831\n",
      "Epoch: 19 | Train Loss: 0.872\n",
      "Epoch: 20 | Train Loss: 0.697\n",
      "Epoch: 20 | Train Loss: 0.584\n",
      "Epoch: 20 | Train Loss: 0.719\n",
      "Epoch: 20 | Train Loss: 0.635\n",
      "Epoch: 20 | Train Loss: 0.636\n",
      "Epoch: 20 | Train Loss: 0.614\n",
      "Epoch: 20 | Train Loss: 0.538\n",
      "Epoch: 20 | Train Loss: 0.613\n",
      "Epoch: 20 | Train Loss: 0.656\n",
      "Epoch: 20 | Train Loss: 0.648\n",
      "Epoch: 20 | Train Loss: 0.672\n",
      "Epoch: 20 | Train Loss: 0.571\n",
      "Epoch: 20 | Train Loss: 0.668\n",
      "Epoch: 20 | Train Loss: 0.602\n",
      "Epoch: 20 | Train Loss: 0.613\n",
      "Epoch: 20 | Train Loss: 0.654\n",
      "Epoch: 20 | Train Loss: 0.647\n",
      "Epoch: 20 | Train Loss: 0.673\n",
      "Epoch: 20 | Train Loss: 0.703\n",
      "Epoch: 20 | Train Loss: 0.652\n",
      "Epoch: 20 | Train Loss: 0.728\n",
      "Epoch: 20 | Train Loss: 0.722\n",
      "Epoch: 20 | Train Loss: 0.741\n",
      "Epoch: 20 | Train Loss: 0.697\n",
      "Epoch: 20 | Train Loss: 0.686\n",
      "Epoch: 20 | Train Loss: 0.641\n",
      "Epoch: 20 | Train Loss: 0.691\n",
      "Epoch: 20 | Train Loss: 0.689\n",
      "Epoch: 20 | Train Loss: 0.743\n",
      "Epoch: 20 | Train Loss: 0.677\n",
      "Epoch: 20 | Train Loss: 0.805\n",
      "Epoch: 20 | Train Loss: 0.741\n",
      "Epoch: 20 | Train Loss: 0.690\n",
      "Epoch: 20 | Train Loss: 0.724\n",
      "Epoch: 20 | Train Loss: 0.660\n",
      "Epoch: 20 | Train Loss: 0.665\n",
      "Epoch: 20 | Train Loss: 0.664\n",
      "Epoch: 20 | Train Loss: 0.625\n",
      "Epoch: 20 | Train Loss: 0.731\n",
      "Epoch: 20 | Train Loss: 0.688\n",
      "Epoch: 20 | Train Loss: 0.705\n",
      "Epoch: 20 | Train Loss: 0.739\n",
      "Epoch: 20 | Train Loss: 0.628\n",
      "Epoch: 20 | Train Loss: 0.708\n",
      "Epoch: 20 | Train Loss: 0.781\n",
      "Epoch: 20 | Train Loss: 0.726\n",
      "Epoch: 20 | Train Loss: 0.689\n",
      "Epoch: 20 | Train Loss: 0.654\n",
      "Epoch: 20 | Train Loss: 0.716\n",
      "Epoch: 20 | Train Loss: 0.689\n",
      "Epoch: 20 | Train Loss: 0.665\n",
      "Epoch: 20 | Train Loss: 0.776\n",
      "Epoch: 20 | Train Loss: 0.746\n",
      "Epoch: 20 | Train Loss: 0.836\n",
      "Epoch: 20 | Train Loss: 0.683\n",
      "Epoch: 20 | Train Loss: 0.699\n",
      "Epoch: 20 | Train Loss: 0.744\n",
      "Epoch: 20 | Train Loss: 0.765\n",
      "Epoch: 20 | Train Loss: 0.760\n",
      "Epoch: 20 | Train Loss: 0.768\n",
      "Epoch: 20 | Train Loss: 0.783\n",
      "Epoch: 20 | Train Loss: 0.750\n",
      "Epoch: 20 | Train Loss: 0.787\n",
      "Epoch: 20 | Train Loss: 0.700\n",
      "Epoch: 20 | Train Loss: 0.767\n",
      "Epoch: 20 | Train Loss: 0.742\n",
      "Epoch: 20 | Train Loss: 0.708\n",
      "Epoch: 21 | Train Loss: 0.569\n",
      "Epoch: 21 | Train Loss: 0.597\n",
      "Epoch: 21 | Train Loss: 0.502\n",
      "Epoch: 21 | Train Loss: 0.596\n",
      "Epoch: 21 | Train Loss: 0.518\n",
      "Epoch: 21 | Train Loss: 0.496\n",
      "Epoch: 21 | Train Loss: 0.547\n",
      "Epoch: 21 | Train Loss: 0.559\n",
      "Epoch: 21 | Train Loss: 0.505\n",
      "Epoch: 21 | Train Loss: 0.565\n",
      "Epoch: 21 | Train Loss: 0.539\n",
      "Epoch: 21 | Train Loss: 0.563\n",
      "Epoch: 21 | Train Loss: 0.563\n",
      "Epoch: 21 | Train Loss: 0.576\n",
      "Epoch: 21 | Train Loss: 0.638\n",
      "Epoch: 21 | Train Loss: 0.539\n",
      "Epoch: 21 | Train Loss: 0.674\n",
      "Epoch: 21 | Train Loss: 0.661\n",
      "Epoch: 21 | Train Loss: 0.540\n",
      "Epoch: 21 | Train Loss: 0.596\n",
      "Epoch: 21 | Train Loss: 0.566\n",
      "Epoch: 21 | Train Loss: 0.512\n",
      "Epoch: 21 | Train Loss: 0.581\n",
      "Epoch: 21 | Train Loss: 0.558\n",
      "Epoch: 21 | Train Loss: 0.601\n",
      "Epoch: 21 | Train Loss: 0.574\n",
      "Epoch: 21 | Train Loss: 0.640\n",
      "Epoch: 21 | Train Loss: 0.615\n",
      "Epoch: 21 | Train Loss: 0.639\n",
      "Epoch: 21 | Train Loss: 0.662\n",
      "Epoch: 21 | Train Loss: 0.584\n",
      "Epoch: 21 | Train Loss: 0.632\n",
      "Epoch: 21 | Train Loss: 0.575\n",
      "Epoch: 21 | Train Loss: 0.576\n",
      "Epoch: 21 | Train Loss: 0.593\n",
      "Epoch: 21 | Train Loss: 0.624\n",
      "Epoch: 21 | Train Loss: 0.637\n",
      "Epoch: 21 | Train Loss: 0.596\n",
      "Epoch: 21 | Train Loss: 0.654\n",
      "Epoch: 21 | Train Loss: 0.664\n",
      "Epoch: 21 | Train Loss: 0.581\n",
      "Epoch: 21 | Train Loss: 0.660\n",
      "Epoch: 21 | Train Loss: 0.605\n",
      "Epoch: 21 | Train Loss: 0.611\n",
      "Epoch: 21 | Train Loss: 0.579\n",
      "Epoch: 21 | Train Loss: 0.613\n",
      "Epoch: 21 | Train Loss: 0.580\n",
      "Epoch: 21 | Train Loss: 0.623\n",
      "Epoch: 21 | Train Loss: 0.569\n",
      "Epoch: 21 | Train Loss: 0.611\n",
      "Epoch: 21 | Train Loss: 0.689\n",
      "Epoch: 21 | Train Loss: 0.549\n",
      "Epoch: 21 | Train Loss: 0.561\n",
      "Epoch: 21 | Train Loss: 0.692\n",
      "Epoch: 21 | Train Loss: 0.681\n",
      "Epoch: 21 | Train Loss: 0.605\n",
      "Epoch: 21 | Train Loss: 0.590\n",
      "Epoch: 21 | Train Loss: 0.737\n",
      "Epoch: 21 | Train Loss: 0.606\n",
      "Epoch: 21 | Train Loss: 0.634\n",
      "Epoch: 21 | Train Loss: 0.665\n",
      "Epoch: 21 | Train Loss: 0.679\n",
      "Epoch: 21 | Train Loss: 0.631\n",
      "Epoch: 21 | Train Loss: 0.652\n",
      "Epoch: 21 | Train Loss: 0.665\n",
      "Epoch: 21 | Train Loss: 0.569\n",
      "Epoch: 21 | Train Loss: 0.620\n",
      "Epoch: 22 | Train Loss: 0.431\n",
      "Epoch: 22 | Train Loss: 0.452\n",
      "Epoch: 22 | Train Loss: 0.507\n",
      "Epoch: 22 | Train Loss: 0.431\n",
      "Epoch: 22 | Train Loss: 0.499\n",
      "Epoch: 22 | Train Loss: 0.448\n",
      "Epoch: 22 | Train Loss: 0.511\n",
      "Epoch: 22 | Train Loss: 0.544\n",
      "Epoch: 22 | Train Loss: 0.487\n",
      "Epoch: 22 | Train Loss: 0.501\n",
      "Epoch: 22 | Train Loss: 0.451\n",
      "Epoch: 22 | Train Loss: 0.491\n",
      "Epoch: 22 | Train Loss: 0.523\n",
      "Epoch: 22 | Train Loss: 0.438\n",
      "Epoch: 22 | Train Loss: 0.464\n",
      "Epoch: 22 | Train Loss: 0.503\n",
      "Epoch: 22 | Train Loss: 0.493\n",
      "Epoch: 22 | Train Loss: 0.521\n",
      "Epoch: 22 | Train Loss: 0.546\n",
      "Epoch: 22 | Train Loss: 0.549\n",
      "Epoch: 22 | Train Loss: 0.497\n",
      "Epoch: 22 | Train Loss: 0.464\n",
      "Epoch: 22 | Train Loss: 0.484\n",
      "Epoch: 22 | Train Loss: 0.506\n",
      "Epoch: 22 | Train Loss: 0.576\n",
      "Epoch: 22 | Train Loss: 0.519\n",
      "Epoch: 22 | Train Loss: 0.502\n",
      "Epoch: 22 | Train Loss: 0.492\n",
      "Epoch: 22 | Train Loss: 0.508\n",
      "Epoch: 22 | Train Loss: 0.502\n",
      "Epoch: 22 | Train Loss: 0.496\n",
      "Epoch: 22 | Train Loss: 0.501\n",
      "Epoch: 22 | Train Loss: 0.502\n",
      "Epoch: 22 | Train Loss: 0.499\n",
      "Epoch: 22 | Train Loss: 0.513\n",
      "Epoch: 22 | Train Loss: 0.505\n",
      "Epoch: 22 | Train Loss: 0.564\n",
      "Epoch: 22 | Train Loss: 0.493\n",
      "Epoch: 22 | Train Loss: 0.503\n",
      "Epoch: 22 | Train Loss: 0.508\n",
      "Epoch: 22 | Train Loss: 0.484\n",
      "Epoch: 22 | Train Loss: 0.531\n",
      "Epoch: 22 | Train Loss: 0.552\n",
      "Epoch: 22 | Train Loss: 0.579\n",
      "Epoch: 22 | Train Loss: 0.565\n",
      "Epoch: 22 | Train Loss: 0.486\n",
      "Epoch: 22 | Train Loss: 0.510\n",
      "Epoch: 22 | Train Loss: 0.505\n",
      "Epoch: 22 | Train Loss: 0.579\n",
      "Epoch: 22 | Train Loss: 0.518\n",
      "Epoch: 22 | Train Loss: 0.525\n",
      "Epoch: 22 | Train Loss: 0.532\n",
      "Epoch: 22 | Train Loss: 0.548\n",
      "Epoch: 22 | Train Loss: 0.483\n",
      "Epoch: 22 | Train Loss: 0.567\n",
      "Epoch: 22 | Train Loss: 0.488\n",
      "Epoch: 22 | Train Loss: 0.510\n",
      "Epoch: 22 | Train Loss: 0.510\n",
      "Epoch: 22 | Train Loss: 0.539\n",
      "Epoch: 22 | Train Loss: 0.503\n",
      "Epoch: 22 | Train Loss: 0.568\n",
      "Epoch: 22 | Train Loss: 0.578\n",
      "Epoch: 22 | Train Loss: 0.568\n",
      "Epoch: 22 | Train Loss: 0.645\n",
      "Epoch: 22 | Train Loss: 0.605\n",
      "Epoch: 22 | Train Loss: 0.604\n",
      "Epoch: 22 | Train Loss: 0.575\n",
      "Epoch: 23 | Train Loss: 0.412\n",
      "Epoch: 23 | Train Loss: 0.367\n",
      "Epoch: 23 | Train Loss: 0.399\n",
      "Epoch: 23 | Train Loss: 0.359\n",
      "Epoch: 23 | Train Loss: 0.384\n",
      "Epoch: 23 | Train Loss: 0.482\n",
      "Epoch: 23 | Train Loss: 0.500\n",
      "Epoch: 23 | Train Loss: 0.404\n",
      "Epoch: 23 | Train Loss: 0.401\n",
      "Epoch: 23 | Train Loss: 0.422\n",
      "Epoch: 23 | Train Loss: 0.450\n",
      "Epoch: 23 | Train Loss: 0.462\n",
      "Epoch: 23 | Train Loss: 0.444\n",
      "Epoch: 23 | Train Loss: 0.491\n",
      "Epoch: 23 | Train Loss: 0.422\n",
      "Epoch: 23 | Train Loss: 0.463\n",
      "Epoch: 23 | Train Loss: 0.435\n",
      "Epoch: 23 | Train Loss: 0.449\n",
      "Epoch: 23 | Train Loss: 0.466\n",
      "Epoch: 23 | Train Loss: 0.474\n",
      "Epoch: 23 | Train Loss: 0.444\n",
      "Epoch: 23 | Train Loss: 0.428\n",
      "Epoch: 23 | Train Loss: 0.453\n",
      "Epoch: 23 | Train Loss: 0.441\n",
      "Epoch: 23 | Train Loss: 0.463\n",
      "Epoch: 23 | Train Loss: 0.434\n",
      "Epoch: 23 | Train Loss: 0.477\n",
      "Epoch: 23 | Train Loss: 0.464\n",
      "Epoch: 23 | Train Loss: 0.448\n",
      "Epoch: 23 | Train Loss: 0.471\n",
      "Epoch: 23 | Train Loss: 0.418\n",
      "Epoch: 23 | Train Loss: 0.457\n",
      "Epoch: 23 | Train Loss: 0.493\n",
      "Epoch: 23 | Train Loss: 0.407\n",
      "Epoch: 23 | Train Loss: 0.517\n",
      "Epoch: 23 | Train Loss: 0.495\n",
      "Epoch: 23 | Train Loss: 0.480\n",
      "Epoch: 23 | Train Loss: 0.437\n",
      "Epoch: 23 | Train Loss: 0.418\n",
      "Epoch: 23 | Train Loss: 0.437\n",
      "Epoch: 23 | Train Loss: 0.439\n",
      "Epoch: 23 | Train Loss: 0.480\n",
      "Epoch: 23 | Train Loss: 0.457\n",
      "Epoch: 23 | Train Loss: 0.478\n",
      "Epoch: 23 | Train Loss: 0.451\n",
      "Epoch: 23 | Train Loss: 0.465\n",
      "Epoch: 23 | Train Loss: 0.404\n",
      "Epoch: 23 | Train Loss: 0.434\n",
      "Epoch: 23 | Train Loss: 0.454\n",
      "Epoch: 23 | Train Loss: 0.436\n",
      "Epoch: 23 | Train Loss: 0.446\n",
      "Epoch: 23 | Train Loss: 0.476\n",
      "Epoch: 23 | Train Loss: 0.418\n",
      "Epoch: 23 | Train Loss: 0.528\n",
      "Epoch: 23 | Train Loss: 0.476\n",
      "Epoch: 23 | Train Loss: 0.498\n",
      "Epoch: 23 | Train Loss: 0.445\n",
      "Epoch: 23 | Train Loss: 0.474\n",
      "Epoch: 23 | Train Loss: 0.467\n",
      "Epoch: 23 | Train Loss: 0.535\n",
      "Epoch: 23 | Train Loss: 0.572\n",
      "Epoch: 23 | Train Loss: 0.490\n",
      "Epoch: 23 | Train Loss: 0.510\n",
      "Epoch: 23 | Train Loss: 0.509\n",
      "Epoch: 23 | Train Loss: 0.439\n",
      "Epoch: 23 | Train Loss: 0.476\n",
      "Epoch: 23 | Train Loss: 0.463\n",
      "Epoch: 24 | Train Loss: 0.429\n",
      "Epoch: 24 | Train Loss: 0.320\n",
      "Epoch: 24 | Train Loss: 0.341\n",
      "Epoch: 24 | Train Loss: 0.439\n",
      "Epoch: 24 | Train Loss: 0.378\n",
      "Epoch: 24 | Train Loss: 0.387\n",
      "Epoch: 24 | Train Loss: 0.411\n",
      "Epoch: 24 | Train Loss: 0.380\n",
      "Epoch: 24 | Train Loss: 0.381\n",
      "Epoch: 24 | Train Loss: 0.430\n",
      "Epoch: 24 | Train Loss: 0.372\n",
      "Epoch: 24 | Train Loss: 0.330\n",
      "Epoch: 24 | Train Loss: 0.327\n",
      "Epoch: 24 | Train Loss: 0.380\n",
      "Epoch: 24 | Train Loss: 0.390\n",
      "Epoch: 24 | Train Loss: 0.444\n",
      "Epoch: 24 | Train Loss: 0.379\n",
      "Epoch: 24 | Train Loss: 0.354\n",
      "Epoch: 24 | Train Loss: 0.421\n",
      "Epoch: 24 | Train Loss: 0.352\n",
      "Epoch: 24 | Train Loss: 0.441\n",
      "Epoch: 24 | Train Loss: 0.343\n",
      "Epoch: 24 | Train Loss: 0.369\n",
      "Epoch: 24 | Train Loss: 0.362\n",
      "Epoch: 24 | Train Loss: 0.392\n",
      "Epoch: 24 | Train Loss: 0.394\n",
      "Epoch: 24 | Train Loss: 0.429\n",
      "Epoch: 24 | Train Loss: 0.376\n",
      "Epoch: 24 | Train Loss: 0.394\n",
      "Epoch: 24 | Train Loss: 0.383\n",
      "Epoch: 24 | Train Loss: 0.394\n",
      "Epoch: 24 | Train Loss: 0.400\n",
      "Epoch: 24 | Train Loss: 0.403\n",
      "Epoch: 24 | Train Loss: 0.401\n",
      "Epoch: 24 | Train Loss: 0.409\n",
      "Epoch: 24 | Train Loss: 0.401\n",
      "Epoch: 24 | Train Loss: 0.374\n",
      "Epoch: 24 | Train Loss: 0.374\n",
      "Epoch: 24 | Train Loss: 0.404\n",
      "Epoch: 24 | Train Loss: 0.355\n",
      "Epoch: 24 | Train Loss: 0.391\n",
      "Epoch: 24 | Train Loss: 0.431\n",
      "Epoch: 24 | Train Loss: 0.383\n",
      "Epoch: 24 | Train Loss: 0.444\n",
      "Epoch: 24 | Train Loss: 0.398\n",
      "Epoch: 24 | Train Loss: 0.426\n",
      "Epoch: 24 | Train Loss: 0.418\n",
      "Epoch: 24 | Train Loss: 0.339\n",
      "Epoch: 24 | Train Loss: 0.377\n",
      "Epoch: 24 | Train Loss: 0.425\n",
      "Epoch: 24 | Train Loss: 0.405\n",
      "Epoch: 24 | Train Loss: 0.477\n",
      "Epoch: 24 | Train Loss: 0.389\n",
      "Epoch: 24 | Train Loss: 0.426\n",
      "Epoch: 24 | Train Loss: 0.432\n",
      "Epoch: 24 | Train Loss: 0.391\n",
      "Epoch: 24 | Train Loss: 0.465\n",
      "Epoch: 24 | Train Loss: 0.428\n",
      "Epoch: 24 | Train Loss: 0.414\n",
      "Epoch: 24 | Train Loss: 0.420\n",
      "Epoch: 24 | Train Loss: 0.477\n",
      "Epoch: 24 | Train Loss: 0.424\n",
      "Epoch: 24 | Train Loss: 0.428\n",
      "Epoch: 24 | Train Loss: 0.430\n",
      "Epoch: 24 | Train Loss: 0.409\n",
      "Epoch: 24 | Train Loss: 0.440\n",
      "Epoch: 24 | Train Loss: 0.560\n",
      "Epoch: 25 | Train Loss: 0.321\n",
      "Epoch: 25 | Train Loss: 0.320\n",
      "Epoch: 25 | Train Loss: 0.310\n",
      "Epoch: 25 | Train Loss: 0.316\n",
      "Epoch: 25 | Train Loss: 0.332\n",
      "Epoch: 25 | Train Loss: 0.309\n",
      "Epoch: 25 | Train Loss: 0.326\n",
      "Epoch: 25 | Train Loss: 0.414\n",
      "Epoch: 25 | Train Loss: 0.303\n",
      "Epoch: 25 | Train Loss: 0.327\n",
      "Epoch: 25 | Train Loss: 0.332\n",
      "Epoch: 25 | Train Loss: 0.317\n",
      "Epoch: 25 | Train Loss: 0.314\n",
      "Epoch: 25 | Train Loss: 0.374\n",
      "Epoch: 25 | Train Loss: 0.309\n",
      "Epoch: 25 | Train Loss: 0.310\n",
      "Epoch: 25 | Train Loss: 0.308\n",
      "Epoch: 25 | Train Loss: 0.358\n",
      "Epoch: 25 | Train Loss: 0.362\n",
      "Epoch: 25 | Train Loss: 0.356\n",
      "Epoch: 25 | Train Loss: 0.292\n",
      "Epoch: 25 | Train Loss: 0.319\n",
      "Epoch: 25 | Train Loss: 0.337\n",
      "Epoch: 25 | Train Loss: 0.333\n",
      "Epoch: 25 | Train Loss: 0.343\n",
      "Epoch: 25 | Train Loss: 0.329\n",
      "Epoch: 25 | Train Loss: 0.350\n",
      "Epoch: 25 | Train Loss: 0.356\n",
      "Epoch: 25 | Train Loss: 0.327\n",
      "Epoch: 25 | Train Loss: 0.386\n",
      "Epoch: 25 | Train Loss: 0.374\n",
      "Epoch: 25 | Train Loss: 0.278\n",
      "Epoch: 25 | Train Loss: 0.341\n",
      "Epoch: 25 | Train Loss: 0.392\n",
      "Epoch: 25 | Train Loss: 0.356\n",
      "Epoch: 25 | Train Loss: 0.340\n",
      "Epoch: 25 | Train Loss: 0.365\n",
      "Epoch: 25 | Train Loss: 0.362\n",
      "Epoch: 25 | Train Loss: 0.359\n",
      "Epoch: 25 | Train Loss: 0.357\n",
      "Epoch: 25 | Train Loss: 0.338\n",
      "Epoch: 25 | Train Loss: 0.374\n",
      "Epoch: 25 | Train Loss: 0.310\n",
      "Epoch: 25 | Train Loss: 0.359\n",
      "Epoch: 25 | Train Loss: 0.334\n",
      "Epoch: 25 | Train Loss: 0.381\n",
      "Epoch: 25 | Train Loss: 0.358\n",
      "Epoch: 25 | Train Loss: 0.347\n",
      "Epoch: 25 | Train Loss: 0.361\n",
      "Epoch: 25 | Train Loss: 0.360\n",
      "Epoch: 25 | Train Loss: 0.427\n",
      "Epoch: 25 | Train Loss: 0.363\n",
      "Epoch: 25 | Train Loss: 0.339\n",
      "Epoch: 25 | Train Loss: 0.370\n",
      "Epoch: 25 | Train Loss: 0.375\n",
      "Epoch: 25 | Train Loss: 0.358\n",
      "Epoch: 25 | Train Loss: 0.365\n",
      "Epoch: 25 | Train Loss: 0.367\n",
      "Epoch: 25 | Train Loss: 0.411\n",
      "Epoch: 25 | Train Loss: 0.373\n",
      "Epoch: 25 | Train Loss: 0.380\n",
      "Epoch: 25 | Train Loss: 0.375\n",
      "Epoch: 25 | Train Loss: 0.376\n",
      "Epoch: 25 | Train Loss: 0.394\n",
      "Epoch: 25 | Train Loss: 0.384\n",
      "Epoch: 25 | Train Loss: 0.431\n",
      "Epoch: 25 | Train Loss: 0.367\n",
      "Epoch: 26 | Train Loss: 0.221\n",
      "Epoch: 26 | Train Loss: 0.264\n",
      "Epoch: 26 | Train Loss: 0.287\n",
      "Epoch: 26 | Train Loss: 0.308\n",
      "Epoch: 26 | Train Loss: 0.326\n",
      "Epoch: 26 | Train Loss: 0.310\n",
      "Epoch: 26 | Train Loss: 0.238\n",
      "Epoch: 26 | Train Loss: 0.286\n",
      "Epoch: 26 | Train Loss: 0.320\n",
      "Epoch: 26 | Train Loss: 0.261\n",
      "Epoch: 26 | Train Loss: 0.283\n",
      "Epoch: 26 | Train Loss: 0.284\n",
      "Epoch: 26 | Train Loss: 0.283\n",
      "Epoch: 26 | Train Loss: 0.278\n",
      "Epoch: 26 | Train Loss: 0.284\n",
      "Epoch: 26 | Train Loss: 0.277\n",
      "Epoch: 26 | Train Loss: 0.316\n",
      "Epoch: 26 | Train Loss: 0.290\n",
      "Epoch: 26 | Train Loss: 0.261\n",
      "Epoch: 26 | Train Loss: 0.298\n",
      "Epoch: 26 | Train Loss: 0.273\n",
      "Epoch: 26 | Train Loss: 0.312\n",
      "Epoch: 26 | Train Loss: 0.300\n",
      "Epoch: 26 | Train Loss: 0.289\n",
      "Epoch: 26 | Train Loss: 0.327\n",
      "Epoch: 26 | Train Loss: 0.279\n",
      "Epoch: 26 | Train Loss: 0.297\n",
      "Epoch: 26 | Train Loss: 0.272\n",
      "Epoch: 26 | Train Loss: 0.278\n",
      "Epoch: 26 | Train Loss: 0.284\n",
      "Epoch: 26 | Train Loss: 0.281\n",
      "Epoch: 26 | Train Loss: 0.259\n",
      "Epoch: 26 | Train Loss: 0.310\n",
      "Epoch: 26 | Train Loss: 0.291\n",
      "Epoch: 26 | Train Loss: 0.322\n",
      "Epoch: 26 | Train Loss: 0.269\n",
      "Epoch: 26 | Train Loss: 0.291\n",
      "Epoch: 26 | Train Loss: 0.307\n",
      "Epoch: 26 | Train Loss: 0.337\n",
      "Epoch: 26 | Train Loss: 0.305\n",
      "Epoch: 26 | Train Loss: 0.287\n",
      "Epoch: 26 | Train Loss: 0.272\n",
      "Epoch: 26 | Train Loss: 0.331\n",
      "Epoch: 26 | Train Loss: 0.319\n",
      "Epoch: 26 | Train Loss: 0.318\n",
      "Epoch: 26 | Train Loss: 0.300\n",
      "Epoch: 26 | Train Loss: 0.307\n",
      "Epoch: 26 | Train Loss: 0.294\n",
      "Epoch: 26 | Train Loss: 0.264\n",
      "Epoch: 26 | Train Loss: 0.380\n",
      "Epoch: 26 | Train Loss: 0.288\n",
      "Epoch: 26 | Train Loss: 0.285\n",
      "Epoch: 26 | Train Loss: 0.360\n",
      "Epoch: 26 | Train Loss: 0.324\n",
      "Epoch: 26 | Train Loss: 0.281\n",
      "Epoch: 26 | Train Loss: 0.307\n",
      "Epoch: 26 | Train Loss: 0.321\n",
      "Epoch: 26 | Train Loss: 0.303\n",
      "Epoch: 26 | Train Loss: 0.303\n",
      "Epoch: 26 | Train Loss: 0.312\n",
      "Epoch: 26 | Train Loss: 0.331\n",
      "Epoch: 26 | Train Loss: 0.292\n",
      "Epoch: 26 | Train Loss: 0.304\n",
      "Epoch: 26 | Train Loss: 0.315\n",
      "Epoch: 26 | Train Loss: 0.320\n",
      "Epoch: 26 | Train Loss: 0.358\n",
      "Epoch: 26 | Train Loss: 0.367\n",
      "Epoch: 27 | Train Loss: 0.215\n",
      "Epoch: 27 | Train Loss: 0.270\n",
      "Epoch: 27 | Train Loss: 0.250\n",
      "Epoch: 27 | Train Loss: 0.226\n",
      "Epoch: 27 | Train Loss: 0.219\n",
      "Epoch: 27 | Train Loss: 0.282\n",
      "Epoch: 27 | Train Loss: 0.235\n",
      "Epoch: 27 | Train Loss: 0.217\n",
      "Epoch: 27 | Train Loss: 0.257\n",
      "Epoch: 27 | Train Loss: 0.253\n",
      "Epoch: 27 | Train Loss: 0.268\n",
      "Epoch: 27 | Train Loss: 0.242\n",
      "Epoch: 27 | Train Loss: 0.264\n",
      "Epoch: 27 | Train Loss: 0.245\n",
      "Epoch: 27 | Train Loss: 0.228\n",
      "Epoch: 27 | Train Loss: 0.210\n",
      "Epoch: 27 | Train Loss: 0.236\n",
      "Epoch: 27 | Train Loss: 0.288\n",
      "Epoch: 27 | Train Loss: 0.224\n",
      "Epoch: 27 | Train Loss: 0.244\n",
      "Epoch: 27 | Train Loss: 0.273\n",
      "Epoch: 27 | Train Loss: 0.301\n",
      "Epoch: 27 | Train Loss: 0.280\n",
      "Epoch: 27 | Train Loss: 0.248\n",
      "Epoch: 27 | Train Loss: 0.233\n",
      "Epoch: 27 | Train Loss: 0.264\n",
      "Epoch: 27 | Train Loss: 0.272\n",
      "Epoch: 27 | Train Loss: 0.276\n",
      "Epoch: 27 | Train Loss: 0.264\n",
      "Epoch: 27 | Train Loss: 0.266\n",
      "Epoch: 27 | Train Loss: 0.277\n",
      "Epoch: 27 | Train Loss: 0.300\n",
      "Epoch: 27 | Train Loss: 0.301\n",
      "Epoch: 27 | Train Loss: 0.247\n",
      "Epoch: 27 | Train Loss: 0.265\n",
      "Epoch: 27 | Train Loss: 0.284\n",
      "Epoch: 27 | Train Loss: 0.257\n",
      "Epoch: 27 | Train Loss: 0.250\n",
      "Epoch: 27 | Train Loss: 0.244\n",
      "Epoch: 27 | Train Loss: 0.265\n",
      "Epoch: 27 | Train Loss: 0.270\n",
      "Epoch: 27 | Train Loss: 0.245\n",
      "Epoch: 27 | Train Loss: 0.263\n",
      "Epoch: 27 | Train Loss: 0.245\n",
      "Epoch: 27 | Train Loss: 0.254\n",
      "Epoch: 27 | Train Loss: 0.261\n",
      "Epoch: 27 | Train Loss: 0.251\n",
      "Epoch: 27 | Train Loss: 0.281\n",
      "Epoch: 27 | Train Loss: 0.288\n",
      "Epoch: 27 | Train Loss: 0.296\n",
      "Epoch: 27 | Train Loss: 0.293\n",
      "Epoch: 27 | Train Loss: 0.261\n",
      "Epoch: 27 | Train Loss: 0.276\n",
      "Epoch: 27 | Train Loss: 0.311\n",
      "Epoch: 27 | Train Loss: 0.250\n",
      "Epoch: 27 | Train Loss: 0.281\n",
      "Epoch: 27 | Train Loss: 0.319\n",
      "Epoch: 27 | Train Loss: 0.285\n",
      "Epoch: 27 | Train Loss: 0.293\n",
      "Epoch: 27 | Train Loss: 0.287\n",
      "Epoch: 27 | Train Loss: 0.286\n",
      "Epoch: 27 | Train Loss: 0.282\n",
      "Epoch: 27 | Train Loss: 0.253\n",
      "Epoch: 27 | Train Loss: 0.332\n",
      "Epoch: 27 | Train Loss: 0.315\n",
      "Epoch: 27 | Train Loss: 0.290\n",
      "Epoch: 27 | Train Loss: 0.311\n",
      "Epoch: 28 | Train Loss: 0.198\n",
      "Epoch: 28 | Train Loss: 0.176\n",
      "Epoch: 28 | Train Loss: 0.202\n",
      "Epoch: 28 | Train Loss: 0.186\n",
      "Epoch: 28 | Train Loss: 0.171\n",
      "Epoch: 28 | Train Loss: 0.218\n",
      "Epoch: 28 | Train Loss: 0.244\n",
      "Epoch: 28 | Train Loss: 0.219\n",
      "Epoch: 28 | Train Loss: 0.251\n",
      "Epoch: 28 | Train Loss: 0.203\n",
      "Epoch: 28 | Train Loss: 0.225\n",
      "Epoch: 28 | Train Loss: 0.208\n",
      "Epoch: 28 | Train Loss: 0.224\n",
      "Epoch: 28 | Train Loss: 0.206\n",
      "Epoch: 28 | Train Loss: 0.225\n",
      "Epoch: 28 | Train Loss: 0.214\n",
      "Epoch: 28 | Train Loss: 0.228\n",
      "Epoch: 28 | Train Loss: 0.204\n",
      "Epoch: 28 | Train Loss: 0.189\n",
      "Epoch: 28 | Train Loss: 0.237\n",
      "Epoch: 28 | Train Loss: 0.276\n",
      "Epoch: 28 | Train Loss: 0.200\n",
      "Epoch: 28 | Train Loss: 0.245\n",
      "Epoch: 28 | Train Loss: 0.220\n",
      "Epoch: 28 | Train Loss: 0.227\n",
      "Epoch: 28 | Train Loss: 0.258\n",
      "Epoch: 28 | Train Loss: 0.232\n",
      "Epoch: 28 | Train Loss: 0.236\n",
      "Epoch: 28 | Train Loss: 0.239\n",
      "Epoch: 28 | Train Loss: 0.232\n",
      "Epoch: 28 | Train Loss: 0.230\n",
      "Epoch: 28 | Train Loss: 0.243\n",
      "Epoch: 28 | Train Loss: 0.225\n",
      "Epoch: 28 | Train Loss: 0.232\n",
      "Epoch: 28 | Train Loss: 0.179\n",
      "Epoch: 28 | Train Loss: 0.219\n",
      "Epoch: 28 | Train Loss: 0.228\n",
      "Epoch: 28 | Train Loss: 0.200\n",
      "Epoch: 28 | Train Loss: 0.206\n",
      "Epoch: 28 | Train Loss: 0.251\n",
      "Epoch: 28 | Train Loss: 0.271\n",
      "Epoch: 28 | Train Loss: 0.235\n",
      "Epoch: 28 | Train Loss: 0.245\n",
      "Epoch: 28 | Train Loss: 0.294\n",
      "Epoch: 28 | Train Loss: 0.236\n",
      "Epoch: 28 | Train Loss: 0.230\n",
      "Epoch: 28 | Train Loss: 0.257\n",
      "Epoch: 28 | Train Loss: 0.231\n",
      "Epoch: 28 | Train Loss: 0.249\n",
      "Epoch: 28 | Train Loss: 0.264\n",
      "Epoch: 28 | Train Loss: 0.285\n",
      "Epoch: 28 | Train Loss: 0.225\n",
      "Epoch: 28 | Train Loss: 0.216\n",
      "Epoch: 28 | Train Loss: 0.223\n",
      "Epoch: 28 | Train Loss: 0.245\n",
      "Epoch: 28 | Train Loss: 0.265\n",
      "Epoch: 28 | Train Loss: 0.281\n",
      "Epoch: 28 | Train Loss: 0.218\n",
      "Epoch: 28 | Train Loss: 0.243\n",
      "Epoch: 28 | Train Loss: 0.260\n",
      "Epoch: 28 | Train Loss: 0.279\n",
      "Epoch: 28 | Train Loss: 0.259\n",
      "Epoch: 28 | Train Loss: 0.264\n",
      "Epoch: 28 | Train Loss: 0.239\n",
      "Epoch: 28 | Train Loss: 0.274\n",
      "Epoch: 28 | Train Loss: 0.229\n",
      "Epoch: 28 | Train Loss: 0.302\n",
      "Epoch: 29 | Train Loss: 0.182\n",
      "Epoch: 29 | Train Loss: 0.185\n",
      "Epoch: 29 | Train Loss: 0.150\n",
      "Epoch: 29 | Train Loss: 0.202\n",
      "Epoch: 29 | Train Loss: 0.200\n",
      "Epoch: 29 | Train Loss: 0.207\n",
      "Epoch: 29 | Train Loss: 0.189\n",
      "Epoch: 29 | Train Loss: 0.141\n",
      "Epoch: 29 | Train Loss: 0.196\n",
      "Epoch: 29 | Train Loss: 0.167\n",
      "Epoch: 29 | Train Loss: 0.232\n",
      "Epoch: 29 | Train Loss: 0.215\n",
      "Epoch: 29 | Train Loss: 0.179\n",
      "Epoch: 29 | Train Loss: 0.212\n",
      "Epoch: 29 | Train Loss: 0.186\n",
      "Epoch: 29 | Train Loss: 0.161\n",
      "Epoch: 29 | Train Loss: 0.192\n",
      "Epoch: 29 | Train Loss: 0.236\n",
      "Epoch: 29 | Train Loss: 0.166\n",
      "Epoch: 29 | Train Loss: 0.175\n",
      "Epoch: 29 | Train Loss: 0.174\n",
      "Epoch: 29 | Train Loss: 0.191\n",
      "Epoch: 29 | Train Loss: 0.186\n",
      "Epoch: 29 | Train Loss: 0.227\n",
      "Epoch: 29 | Train Loss: 0.191\n",
      "Epoch: 29 | Train Loss: 0.207\n",
      "Epoch: 29 | Train Loss: 0.206\n",
      "Epoch: 29 | Train Loss: 0.181\n",
      "Epoch: 29 | Train Loss: 0.222\n",
      "Epoch: 29 | Train Loss: 0.187\n",
      "Epoch: 29 | Train Loss: 0.192\n",
      "Epoch: 29 | Train Loss: 0.210\n",
      "Epoch: 29 | Train Loss: 0.199\n",
      "Epoch: 29 | Train Loss: 0.208\n",
      "Epoch: 29 | Train Loss: 0.214\n",
      "Epoch: 29 | Train Loss: 0.210\n",
      "Epoch: 29 | Train Loss: 0.164\n",
      "Epoch: 29 | Train Loss: 0.206\n",
      "Epoch: 29 | Train Loss: 0.214\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 3\u001b[0m         loss \u001b[38;5;241m=\u001b[39m train(model, j[\u001b[38;5;241m0\u001b[39m], j[\u001b[38;5;241m1\u001b[39m], optimizer, criterion, CLIP)\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, src, trg, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m      7\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[1;32m----> 9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    for i,j in enumerate(train_loader):\n",
    "        loss = train(model, j[0], j[1], optimizer, criterion, CLIP)\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {loss:.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184db10-193d-4235-a53e-84ececb85fe9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for i,j in enumerate(train_loader):\n",
    "    encoderhidden=engenc(j[0])\n",
    "    jptext=j[1]\n",
    "    jptextfirsttok=jptext[:,0]\n",
    "    decodernext,decoderhidden=jpdec(jptextfirsttok,encoderhidden)\n",
    "    print(decodernext.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c830a8b-3550-456f-93f0-c48d1f288e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"LANG TRANSLATION model save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace8566-83ab-4f5f-b24a-bf2b6ac2a789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417bbd2-d538-4622-bf60-f06b8feb4100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47458f3b-d16d-4b07-ba2e-8c70ce711c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence, en_vocab, ja_vocab, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = [en_vocab['<SOS>']] + numericalize(sentence, en_vocab, tokenize_en) + [en_vocab['<EOS>']]\n",
    "    src = torch.LongTensor(tokens).unsqueeze(0).to('cpu') # unsqueeze to (1, seq_len)\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src)\n",
    "    trg_index = [ja_vocab['<SOS>']]\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_index[-1]]).to('cpu')\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden)\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_index.append(pred_token)\n",
    "        if pred_token == ja_vocab['<EOS>']:\n",
    "            break\n",
    "    ja_tokens = [k for k, v in ja_vocab.items() if v in trg_index]\n",
    "    return ''.join(ja_tokens[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "945bf286-2212-4d65-a68b-b4bcef349fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model,sent,en_tokenizer,jp_tokenizer,max_len=45):\n",
    "    ax = en_tokenizer.tokenize(sent)\n",
    "    ax = [en_tokenizer.bos_token] + ax + [en_tokenizer.eos_token] \n",
    "    print(len(ax))\n",
    "    for i in range(45-len(ax)):\n",
    "        ax=ax+[en_tokenizer.pad_token]\n",
    "    print(len(ax))\n",
    "    inptorch=(torch.tensor(en_tokenizer.convert_tokens_to_ids(ax)))\n",
    "    inptorch=inptorch.unsqueeze(0)\n",
    "    print(inptorch)\n",
    "    hidden=model.encoder(inptorch)\n",
    "    target=[jp_tokenizer.bos_token_id]\n",
    "    for _ in range(max_len):\n",
    "        targettensor=torch.LongTensor(target[-1])\n",
    "        print(targettensor.shape,hidden.shape)\n",
    "        output,hidden=model.decoder(targettensor,hidden)\n",
    "        pred_token=output.argmax(1).item()\n",
    "        target.append(pred_token)\n",
    "        if pred_token==jp_tokenizer.eos_token:\n",
    "            break\n",
    "        print(pred_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f62c9ffc-c1a7-4d9f-ba8a-63c27f6baf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "45\n",
      "tensor([[30522,  2054,  2003,  2115,  2171,  1029, 30523, 30524, 30524, 30524,\n",
      "         30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524,\n",
      "         30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524,\n",
      "         30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524,\n",
      "         30524, 30524, 30524, 30524, 30524]])\n",
      "torch.Size([32000]) torch.Size([1, 1, 800])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m translate(model,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is your name?\u001b[39m\u001b[38;5;124m\"\u001b[39m,engtokenizer,tokenizer)\n",
      "Cell \u001b[1;32mIn[89], line 16\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(model, sent, en_tokenizer, jp_tokenizer, max_len)\u001b[0m\n\u001b[0;32m     14\u001b[0m targettensor\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mLongTensor(target[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(targettensor\u001b[38;5;241m.\u001b[39mshape,hidden\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 16\u001b[0m output,hidden\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(targettensor,hidden)\n\u001b[0;32m     17\u001b[0m pred_token\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m target\u001b[38;5;241m.\u001b[39mappend(pred_token)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m, in \u001b[0;36mdeecoder.forward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hidden):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# unsqueeze to (batch_size, 1)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m     13\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embedded, hidden\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# Transpose hidden to (num_layers * num_directions, batch_size, hidden_size)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "translate(model,\"what is your name?\",engtokenizer,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f0df4e1-997f-44f0-a41f-7adf33ff8507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> what is your name? <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engtokenizer.decode([30522,  2054,  2003,  2115,  2171,  1029, 30523, 30524, 30524, 30524,\n",
    "         30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524,\n",
    "         30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524,\n",
    "         30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524, 30524,\n",
    "         30524, 30524, 30524, 30524, 30524])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cbcc8f-a1cb-4eda-bec7-6223845f1d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
